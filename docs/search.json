[
  {
    "objectID": "hw9.html",
    "href": "hw9.html",
    "title": "Assignment 9",
    "section": "",
    "text": "# Reading in data\ndata &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",\n                 local = locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n# Turning the Date column into a real Date\n# Dropping old Date column\ndata &lt;- data |&gt;\n  mutate(date = lubridate::dmy(Date)) |&gt;\n  select(-Date)\n\n# Turning character columns into factor variables\ndata &lt;- data |&gt;\n  mutate(seasons = factor(Seasons),\n         holiday = factor(Holiday),\n         fn_day = factor(`Functioning Day`)) |&gt;\n  select(-Seasons, -Holiday, -`Functioning Day`) # dropping old variables\n\n# Renaming other variables for simplicity\ndata &lt;- data |&gt;\n  rename(bike_count = \"Rented Bike Count\",\n    hour = \"Hour\",\n    temperature = \"Temperature(°C)\",\n    humidity = \"Humidity(%)\",\n    wind_speed = \"Wind speed (m/s)\",\n    visibility = \"Visibility (10m)\",\n    dew_point_temp = \"Dew point temperature(°C)\",\n    solar_radiation = \"Solar Radiation (MJ/m2)\",\n    rainfall = \"Rainfall(mm)\",\n    snowfall = \"Snowfall (cm)\")\n\n# Removing observations where buke rentals were out of commission.\ndata &lt;- data |&gt;\n  filter(fn_day == \"Yes\") |&gt;\n  select(-fn_day)\n\n# group_by() the date, seasons, and holiday variables and find the sum of the bike_count, rainfall, and snowfall variables and the mean of all the weather related variables.\ndata &lt;- data |&gt;\n  group_by(date, seasons, holiday) |&gt;\n  summarize(bike_count = sum(bike_count),\n             temp = mean(temperature),\n             humidity = mean(humidity),\n             wind_speed = mean(wind_speed),\n             vis = mean(visibility),\n             dew_point_temp = mean(dew_point_temp),\n             solar_radiation = mean(solar_radiation),\n             rainfall = sum(rainfall),\n             snowfall = sum(snowfall)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'date', 'seasons'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n# tidy models to split the data into a training and test set (75/25 split)\n# strata argument to stratify the split on the seasons\n\ndata_split &lt;- initial_split(data, prop = 0.75,strata = seasons)\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\n\n# 10 fold cross validation on the training set\ndata_10_fold &lt;- vfold_cv(data_train, 10)\n\n\n\n\n\n# Recipe 1\nrec_1 &lt;- recipe(bike_count ~ ., data = data_split) |&gt;\n  step_date(date, features = \"dow\") |&gt; # extracting \"dow\" column elements\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt; # removing unneeded variables\n  step_dummy(seasons, holiday, day_type) |&gt; # dummy variables for factors\n  step_normalize(all_numeric(), -bike_count) # normalizing all numeric variables besides response variable\n\n# Recipe 2\nrec_2 &lt;- rec_1 |&gt;\n  step_interact(terms = ~starts_with(\"seasons\")*starts_with(\"holiday\") +\n                  starts_with(\"seasons\")*temp +\n                  temp*rainfall) # Interactions for 2nd recipe model\n\n# Recipe 3\nrec_3 &lt;- rec_2 |&gt; \n  step_poly(temp,\n            wind_speed,\n            vis,\n            dew_point_temp,\n            solar_radiation,\n            rainfall,\n            snowfall,\n            degree = 2) # Quadratic terms for each numeric predictor\n\n\n\n\n\n# Linear regression model\nmlr_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n\n\n\n\n# Model 1\nmlr_fit_1 &lt;- workflow() |&gt;\n  add_recipe(rec_1) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  fit_resamples(data_10_fold) # 10 fold CV\n\n# Model 2\nmlr_fit_2 &lt;- workflow() |&gt;\n  add_recipe(rec_2) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  fit_resamples(data_10_fold) # 10 fold CV\n\n# Model 3\nmlr_fit_3 &lt;- workflow() |&gt;\n  add_recipe(rec_3) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  fit_resamples(data_10_fold) # 10 fold CV\n\n\n\n\n\n# Collecting model metrics for all three fitted MLR models\nrbind(mlr_fit_1 |&gt; collect_metrics(),\n      mlr_fit_2 |&gt; collect_metrics(),\n      mlr_fit_3 |&gt; collect_metrics())\n\n# A tibble: 6 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   3989.       10 228.     Preprocessor1_Model1\n2 rsq     standard      0.838    10   0.0184 Preprocessor1_Model1\n3 rmse    standard   3047.       10 219.     Preprocessor1_Model1\n4 rsq     standard      0.910    10   0.0154 Preprocessor1_Model1\n5 rmse    standard   2987.       10 214.     Preprocessor1_Model1\n6 rsq     standard      0.912    10   0.0166 Preprocessor1_Model1\n\n\nBest model is model 3: RMSE = 2784.05\n\n\n\n\nmlr_final_fit &lt;- workflow() |&gt;\n  add_recipe(rec_3) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\nmlr_final_fit |&gt;\n  collect_metrics() # Collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3088. Preprocessor1_Model1\n2 mae     standard       2342. Preprocessor1_Model1\n\n\n\n\n\n\n# Extract the final fitted parsnip model\nmlr_model &lt;- mlr_final_fit |&gt; extract_fit_parsnip()\n\n# View the model details\nmlr_model\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                        (Intercept)                             humidity  \n                            20655.3                              -2774.7  \n                     seasons_Spring                       seasons_Summer  \n                            -1687.1                               6307.7  \n                     seasons_Winter                   holiday_No.Holiday  \n                            -4822.3                                843.1  \n                   day_type_Weekend  seasons_Spring_x_holiday_No.Holiday  \n                            -1059.2                               -911.5  \nseasons_Summer_x_holiday_No.Holiday  seasons_Winter_x_holiday_No.Holiday  \n                             -571.7                               -701.9  \n              seasons_Spring_x_temp                seasons_Summer_x_temp  \n                             2240.3                              -6597.7  \n              seasons_Winter_x_temp                      temp_x_rainfall  \n                            -2406.1                              -1205.8  \n                        temp_poly_1                          temp_poly_2  \n                           -79670.6                             -10380.3  \n                  wind_speed_poly_1                    wind_speed_poly_2  \n                            -5471.0                               1713.4  \n                         vis_poly_1                           vis_poly_2  \n                             3568.1                               -128.6  \n              dew_point_temp_poly_1                dew_point_temp_poly_2  \n                           137782.2                             -14142.9  \n             solar_radiation_poly_1               solar_radiation_poly_2  \n                            45919.0                              -6091.8  \n                    rainfall_poly_1                      rainfall_poly_2  \n                           -32057.8                               8062.5  \n                    snowfall_poly_1                      snowfall_poly_2  \n                            -2194.0                              -1621.1  \n\n\n\n\n\n\ncoefficients &lt;- broom::tidy(mlr_model)\ncoefficients \n\n# A tibble: 28 × 5\n   term                                estimate std.error statistic  p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                           20655.     1700.     12.2  1.09e-26\n 2 humidity                              -2775.     1332.     -2.08 3.83e- 2\n 3 seasons_Spring                        -1687.      249.     -6.77 1.00e-10\n 4 seasons_Summer                         6308.     1018.      6.20 2.53e- 9\n 5 seasons_Winter                        -4822.     1084.     -4.45 1.35e- 5\n 6 holiday_No.Holiday                      843.      208.      4.06 6.69e- 5\n 7 day_type_Weekend                      -1059.      178.     -5.94 1.03e- 8\n 8 seasons_Spring_x_holiday_No.Holiday    -912.      291.     -3.13 1.97e- 3\n 9 seasons_Summer_x_holiday_No.Holiday    -572.      282.     -2.03 4.34e- 2\n10 seasons_Winter_x_holiday_No.Holiday    -702.      226.     -3.10 2.17e- 3\n# ℹ 18 more rows\n\n\nFor the following models (LASSO, Regression Tree, Bagged Tree, and Random Forest) we will be using recipe 1 when fitting the model workflow.\n\n\n\n\n\n\n\nLASSO_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\n\n\n\n\n# LASSO model 1\nLASSO_wkf &lt;- workflow() |&gt;\n  add_recipe(rec_1) |&gt; # Recipe\n  add_model(LASSO_spec) # Defined model instance\n\n\n\n\n\nLASSO_grid &lt;- LASSO_wkf |&gt;\n  tune_grid(resamples = data_10_fold,\n            grid = grid_regular(penalty(), levels = 200),\n            metrics = metric_set(rmse, mae)) # defining model metrics\n\nWarning: package 'glmnet' was built under R version 4.4.2\n\n\n\n\n\n\nLASSO_grid |&gt;\n  collect_metrics() |&gt; # collecting defining model metrics\n  filter(.metric == \"rmse\") # filter by defining model metrics\n\n# A tibble: 200 × 7\n    penalty .metric .estimator  mean     n std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   3980.    10    223. Preprocessor1_Model001\n 2 1.12e-10 rmse    standard   3980.    10    223. Preprocessor1_Model002\n 3 1.26e-10 rmse    standard   3980.    10    223. Preprocessor1_Model003\n 4 1.41e-10 rmse    standard   3980.    10    223. Preprocessor1_Model004\n 5 1.59e-10 rmse    standard   3980.    10    223. Preprocessor1_Model005\n 6 1.78e-10 rmse    standard   3980.    10    223. Preprocessor1_Model006\n 7 2.00e-10 rmse    standard   3980.    10    223. Preprocessor1_Model007\n 8 2.25e-10 rmse    standard   3980.    10    223. Preprocessor1_Model008\n 9 2.52e-10 rmse    standard   3980.    10    223. Preprocessor1_Model009\n10 2.83e-10 rmse    standard   3980.    10    223. Preprocessor1_Model010\n# ℹ 190 more rows\n\n\n\n\n\n\nlowest_rmse &lt;- LASSO_grid |&gt;\n  select_best(metric = \"rmse\")\n\n\n\n\n\nLASSO_final_fit &lt;- LASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse) |&gt;\n  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\nLASSO_final_fit |&gt;\n  collect_metrics() # collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4627. Preprocessor1_Model1\n2 mae     standard       3363. Preprocessor1_Model1\n\n\n\n\n\n\n# Extract the final model fits\nlasso_model &lt;- LASSO_final_fit |&gt;\n  extract_fit_engine()\n\n# View the final LASSO model details\nlasso_model\n\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev Lambda\n1   0  0.00 7592.0\n2   2 11.28 6918.0\n3   2 21.83 6303.0\n4   2 30.58 5743.0\n5   3 38.18 5233.0\n6   3 44.58 4768.0\n7   3 49.89 4345.0\n8   3 54.29 3959.0\n9   3 57.95 3607.0\n10  3 60.99 3286.0\n11  3 63.52 2995.0\n12  3 65.61 2729.0\n13  3 67.35 2486.0\n14  3 68.79 2265.0\n15  3 69.99 2064.0\n16  4 71.56 1881.0\n17  4 73.04 1714.0\n18  4 74.27 1561.0\n19  4 75.29 1423.0\n20  4 76.13 1296.0\n21  4 76.83 1181.0\n22  4 77.42 1076.0\n23  6 78.14  980.6\n24  6 78.83  893.5\n25  8 79.61  814.1\n26  8 80.39  741.8\n27  8 81.03  675.9\n28  8 81.57  615.8\n29  8 82.01  561.1\n30  8 82.38  511.3\n31  8 82.69  465.9\n32  8 82.94  424.5\n33  8 83.15  386.8\n34  9 83.38  352.4\n35 10 83.78  321.1\n36 10 84.11  292.6\n37 10 84.39  266.6\n38 10 84.62  242.9\n39 10 84.81  221.3\n40 10 84.97  201.7\n41 10 85.10  183.7\n42 10 85.21  167.4\n43 10 85.30  152.5\n44 10 85.37  139.0\n45 10 85.44  126.6\n46 11 85.49  115.4\n47 11 85.53  105.1\n48 11 85.57   95.8\n49 11 85.60   87.3\n50 11 85.63   79.5\n51 11 85.65   72.5\n52 11 85.67   66.0\n53 11 85.68   60.2\n54 11 85.70   54.8\n55 11 85.71   50.0\n56 11 85.72   45.5\n57 11 85.72   41.5\n58 11 85.73   37.8\n59 11 85.74   34.4\n60 12 85.74   31.4\n61 12 85.76   28.6\n62 12 85.78   26.0\n63 13 85.80   23.7\n64 13 85.81   21.6\n65 13 85.83   19.7\n66 13 85.84   18.0\n67 13 85.85   16.4\n68 13 85.85   14.9\n69 13 85.86   13.6\n70 13 85.87   12.4\n71 13 85.87   11.3\n72 13 85.87   10.3\n73 12 85.88    9.4\n74 12 85.88    8.5\n\n\n\n\n\n\nalmost_usual_fit &lt;- extract_fit_parsnip(LASSO_final_fit)\nusual_fit &lt;- almost_usual_fit$fit\nsummary(usual_fit)\n\n          Length Class     Mode   \na0         74    -none-    numeric\nbeta      962    dgCMatrix S4     \ndf         74    -none-    numeric\ndim         2    -none-    numeric\nlambda     74    -none-    numeric\ndev.ratio  74    -none-    numeric\nnulldev     1    -none-    numeric\nnpasses     1    -none-    numeric\njerr        1    -none-    numeric\noffset      1    -none-    logical\ncall        5    -none-    call   \nnobs        1    -none-    numeric\n\n\n\n\n\n\n\n\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n\n\n\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(rec_1) |&gt; # Recipe\n  add_model(tree_mod) # Defined model instance\n\n\n\n\n\ntree_fit &lt;- tree_wkf |&gt; \n  tune_grid(resamples = data_10_fold, # Cross-validation defined earlier\n            metrics = metric_set(rmse, mae)) # defining model metrics\n\n\n\n\n\ntree_fit |&gt;\n  collect_metrics() # collecting defined model metrics\n\n# A tibble: 20 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1        4.20e- 3          3 mae     standard   3519.    10    220. Preprocess…\n 2        4.20e- 3          3 rmse    standard   4463.    10    240. Preprocess…\n 3        3.89e- 6          6 mae     standard   2936.    10    191. Preprocess…\n 4        3.89e- 6          6 rmse    standard   3791.    10    248. Preprocess…\n 5        2.31e- 6          1 mae     standard   5275.    10    235. Preprocess…\n 6        2.31e- 6          1 rmse    standard   6854.    10    369. Preprocess…\n 7        2.57e- 5         11 mae     standard   2853.    10    147. Preprocess…\n 8        2.57e- 5         11 rmse    standard   3733.    10    210. Preprocess…\n 9        7.93e- 8          8 mae     standard   2859.    10    145. Preprocess…\n10        7.93e- 8          8 rmse    standard   3736.    10    205. Preprocess…\n11        8.29e-10          7 mae     standard   2834.    10    170. Preprocess…\n12        8.29e-10          7 rmse    standard   3738.    10    219. Preprocess…\n13        1.08e- 3          4 mae     standard   3396.    10    186. Preprocess…\n14        1.08e- 3          4 rmse    standard   4383.    10    234. Preprocess…\n15        4.83e- 8         10 mae     standard   2853.    10    147. Preprocess…\n16        4.83e- 8         10 rmse    standard   3733.    10    210. Preprocess…\n17        4.75e- 2         14 mae     standard   3938.    10    216. Preprocess…\n18        4.75e- 2         14 rmse    standard   4991.    10    219. Preprocess…\n19        2.71e-10         13 mae     standard   2853.    10    147. Preprocess…\n20        2.71e-10         13 rmse    standard   3733.    10    210. Preprocess…\n\n\n\n\n\n\ntree_best_params &lt;- tree_fit |&gt;\n  select_best(metric = \"rmse\") # defining the metric\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1       0.0000257         11 Preprocessor1_Model04\n\n\n\n\n\n\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\n\n\n\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\ntree_final_fit |&gt;\n  collect_metrics() # Collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4146. Preprocessor1_Model1\n2 mae     standard       3256. Preprocessor1_Model1\n\n\n\n\n\n\n# final model fits\ntree_final_model &lt;- extract_workflow(tree_final_fit) \ntree_final_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 263 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n  1) root 263 26207600000 17770.100  \n    2) temp&lt; -0.2624479 106  3126163000  8597.519  \n      4) seasons_Winter&gt;=0.561781 67   227655600  5406.627  \n        8) temp&lt; -1.542755 18    18881450  4219.556 *\n        9) temp&gt;=-1.542755 49   174092100  5842.694  \n         18) holiday_No.Holiday&lt; -1.98631 8    17059420  3759.000 *\n         19) holiday_No.Holiday&gt;=-1.98631 41   115521000  6249.268  \n           38) rainfall&gt;=-0.298057 10    19775620  4609.600 *\n           39) rainfall&lt; -0.298057 31    60187680  6778.194  \n             78) dew_point_temp&lt; -1.090833 20    38513810  6368.450  \n              156) vis&lt; 0.46653 10    12212740  5832.400 *\n              157) vis&gt;=0.46653 10    20554080  6904.500 *\n             79) dew_point_temp&gt;=-1.090833 11    12210980  7523.182 *\n      5) seasons_Winter&lt; 0.561781 39  1044377000 14079.310  \n       10) rainfall&gt;=-0.1395164 7    62808380  6631.143 *\n       11) rainfall&lt; -0.1395164 32   508295800 15708.590  \n         22) seasons_Spring&gt;=0.561781 15    81804200 12539.530 *\n         23) seasons_Spring&lt; 0.561781 17   142926100 18504.820 *\n    3) temp&gt;=-0.2624479 157  8141647000 23963.040  \n      6) solar_radiation&lt; -0.9473537 16   398061000  8440.000 *\n      7) solar_radiation&gt;=-0.9473537 141  3450651000 25724.520  \n       14) solar_radiation&lt; 0.9335255 86  2012574000 24043.870  \n         28) rainfall&gt;=-0.1488423 17   199567500 19683.290 *\n         29) rainfall&lt; -0.1488423 69  1410117000 25118.220  \n           58) temp&lt; 0.2222043 24   430172800 22407.540  \n            116) seasons_Spring&gt;=0.561781 8    77829020 17751.000 *\n            117) seasons_Spring&lt; 0.561781 16    92143210 24735.810 *\n           59) temp&gt;=0.2222043 45   709546300 26563.910  \n            118) temp&gt;=1.431878 13    67625200 23022.920 *\n            119) temp&lt; 1.431878 32   412699900 28002.440  \n              238) temp&lt; 0.4839235 9   113165800 25036.780 *\n              239) temp&gt;=0.4839235 23   189403700 29162.910  \n                478) humidity&gt;=0.2566585 12    43409210 27711.250 *\n                479) humidity&lt; 0.2566585 11    93119780 30746.550 *\n       15) solar_radiation&gt;=0.9335255 55   815330700 28352.450  \n         30) dew_point_temp&lt; 0.1597449 13    90422240 25036.690 *\n         31) dew_point_temp&gt;=0.1597449 42   537743900 29378.760  \n           62) temp&gt;=1.04259 15   133213500 25996.670 *\n           63) temp&lt; 1.04259 27   137630500 31257.700  \n            126) dew_point_temp&lt; 0.5312419 9    16952300 28869.110 *\n            127) dew_point_temp&gt;=0.5312419 18    43655670 32452.000 *\n\n\n\n\n\n\n# Extract the fitted engine \ntree_final_fit_engine &lt;- tree_final_model |&gt; extract_fit_engine()\n\n# Plot the decision tree using rpart.plot\nrpart.plot(tree_final_fit_engine, roundint = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbag_spec &lt;- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |&gt;\n set_engine(\"rpart\") |&gt;\n set_mode(\"regression\")\n\n\n\n\n\nbag_wkf &lt;- workflow() |&gt;\n add_recipe(rec_1) |&gt; # Recipe\n add_model(bag_spec) # Defined model instance\n\n\n\n\n\nbag_fit &lt;- bag_wkf |&gt;\n tune_grid(resamples = data_10_fold, \n           grid = grid_regular(cost_complexity(), \n                               levels = 15), \n           metrics = metric_set(rmse, mae)) # defining model metrics\n\n\n\n\n\nbag_fit |&gt;\n collect_metrics() |&gt; # collecting defined model metrics\n filter(.metric == \"rmse\") |&gt;\n arrange(mean) # Arranged by mean RMSE value\n\n# A tibble: 15 × 7\n   cost_complexity .metric .estimator  mean     n std_err .config              \n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1        1.64e- 7 rmse    standard   3161.    10    155. Preprocessor1_Model06\n 2        2.68e- 4 rmse    standard   3186.    10    182. Preprocessor1_Model11\n 3        1.18e- 3 rmse    standard   3212.    10    175. Preprocessor1_Model12\n 4        6.11e- 5 rmse    standard   3216.    10    165. Preprocessor1_Model10\n 5        8.48e- 9 rmse    standard   3220.    10    181. Preprocessor1_Model04\n 6        4.39e-10 rmse    standard   3229.    10    136. Preprocessor1_Model02\n 7        3.16e- 6 rmse    standard   3247.    10    183. Preprocessor1_Model08\n 8        1.93e- 9 rmse    standard   3286.    10    209. Preprocessor1_Model03\n 9        1.39e- 5 rmse    standard   3289.    10    152. Preprocessor1_Model09\n10        1   e-10 rmse    standard   3295.    10    198. Preprocessor1_Model01\n11        7.20e- 7 rmse    standard   3308.    10    220. Preprocessor1_Model07\n12        3.73e- 8 rmse    standard   3312.    10    248. Preprocessor1_Model05\n13        5.18e- 3 rmse    standard   3420.    10    209. Preprocessor1_Model13\n14        2.28e- 2 rmse    standard   3815.    10    159. Preprocessor1_Model14\n15        1   e- 1 rmse    standard   4575.    10    155. Preprocessor1_Model15\n\n\n\n\n\n\nbag_best_params &lt;- bag_fit |&gt;\n  select_best(metric = \"rmse\")\nbag_best_params\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1     0.000000164 Preprocessor1_Model06\n\n\n\n\n\n\nbag_final_wkf &lt;- bag_wkf |&gt;\n finalize_workflow(bag_best_params)\n\n\n\n\n\nbag_final_fit &lt;- bag_final_wkf |&gt;\n last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\nbag_final_fit |&gt;\n  collect_metrics() # Collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3433. Preprocessor1_Model1\n2 mae     standard       2580. Preprocessor1_Model1\n\n\n\n\n\n\n# final model fits\nbag_final_model &lt;- extract_fit_engine(bag_final_fit) \nbag_final_model\n\nBagged CART (regression with 11 members)\n\nVariable importance scores include:\n\n# A tibble: 13 × 4\n   term                      value  std.error  used\n   &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n 1 temp               16870562538. 817360583.    11\n 2 dew_point_temp     13339222095. 900377817.    11\n 3 solar_radiation    12089267018. 530510065.    11\n 4 seasons_Winter     10960104116. 655774213.    11\n 5 humidity            5763991151. 563552149.    11\n 6 rainfall            3345702917. 287806910.    11\n 7 snowfall            2991482977. 514851687.    11\n 8 wind_speed          1597564355. 271651017.    11\n 9 seasons_Summer      1201710287. 583650229.     8\n10 seasons_Spring       910799554. 156589814.    11\n11 vis                  797546421.  90844925.    11\n12 day_type_Weekend     205077700.  52658416.    10\n13 holiday_No.Holiday   100978457.  57953074.     9\n\n\n\n\n\n\n# Creating variable importance plot\nbag_final_model$imp |&gt;\n mutate(term = factor(term, levels = term)) |&gt;\n ggplot(aes(x = term, y = value)) +\n geom_bar(stat =\"identity\") +\n coord_flip() +\n  labs(title = \"Variable Importance for Bagged Tree Model\",\n       x = \"Features\",\n       y = \"Importance\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n set_engine(\"ranger\", importance = \"impurity\") |&gt;\n set_mode(\"regression\")\n\n\n\n\n\nrf_wkf &lt;- workflow() |&gt;\n add_recipe(rec_1) |&gt; # Recipe\n add_model(rf_spec) # Defined model instance\n\n\n\n\n\nrf_fit &lt;- rf_wkf |&gt;\n tune_grid(resamples = data_10_fold,\n grid = 7,\n metrics = metric_set(rmse, mae)) # defining model metrics\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n\n\n\n\nrf_fit |&gt;\n collect_metrics() |&gt; # Collecting defined model metrics\n filter(.metric == \"rmse\") |&gt;\n arrange(mean) # Arranged by mean RMSE value\n\n# A tibble: 7 × 7\n   mtry .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     9 rmse    standard   2833.    10    148. Preprocessor1_Model3\n2    11 rmse    standard   2850.    10    134. Preprocessor1_Model4\n3     8 rmse    standard   2865.    10    140. Preprocessor1_Model5\n4    13 rmse    standard   2877.    10    129. Preprocessor1_Model7\n5     6 rmse    standard   2880.    10    159. Preprocessor1_Model1\n6     4 rmse    standard   2982.    10    182. Preprocessor1_Model2\n7     2 rmse    standard   3355.    10    204. Preprocessor1_Model6\n\n\n\n\n\n\nrf_best_params &lt;- rf_fit |&gt;\n  select_best(metric = \"rmse\") # defining the metric\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     9 Preprocessor1_Model3\n\n\n\n\n\n\nrf_final_wkf &lt;- rf_wkf |&gt;\n finalize_workflow(rf_best_params)\n\n\n\n\n\nrf_final_fit &lt;- rf_final_wkf |&gt;\n last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\nrf_final_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [263/90]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\nrf_final_fit |&gt;\n  collect_metrics() # Collecting by defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3128. Preprocessor1_Model1\n2 mae     standard       2420. Preprocessor1_Model1\n\n\n\n\n\n\n# final model fits\nrf_final_model &lt;- extract_fit_engine(rf_final_fit) \nrf_final_model\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~9L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      263 \nNumber of independent variables:  13 \nMtry:                             9 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       8006527 \nR squared (OOB):                  0.919958 \n\n\n\n\n\n\n# Extract variable importance and convert to a tibble\nrf_imp &lt;- rf_final_model$variable.importance\nrf_imp_df &lt;- tibble(\n  term = names(rf_imp), # Feature names\n  value = rf_imp # Feature importance values\n)\n\n# Creating variable importance plot\nrf_imp_df |&gt;\n  mutate(term = factor(term, levels = term)) |&gt;\n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Variable Importance for Random Forest Model\",\n       x = \"Features\",\n       y = \"Importance\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n# MLR model \nmlr_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3088. Preprocessor1_Model1\n2 mae     standard       2342. Preprocessor1_Model1\n\n\n\n# tuned LASSO Model\nLASSO_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4627. Preprocessor1_Model1\n2 mae     standard       3363. Preprocessor1_Model1\n\n\n\n# tuned Regression Tree Model\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4146. Preprocessor1_Model1\n2 mae     standard       3256. Preprocessor1_Model1\n\n\n\n# tuned Bagged Tree Model\nbag_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3433. Preprocessor1_Model1\n2 mae     standard       2580. Preprocessor1_Model1\n\n\n\n# tuned Random Forest Model\nrf_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3128. Preprocessor1_Model1\n2 mae     standard       2420. Preprocessor1_Model1\n\n\nIn comparison of all final models: the tuned Random Forest model is the best!\n\n\n\n\n# Fit the model to the entire data set! \nrf_final_fit &lt;- rf_final_wkf |&gt;\n  fit(data)\n\n# Print the final fitted model\nrf_final_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~9L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      353 \nNumber of independent variables:  13 \nMtry:                             9 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       7634549 \nR squared (OOB):                  0.922686"
  },
  {
    "objectID": "hw9.html#dataset-and-analysis-from-previous-assignment.",
    "href": "hw9.html#dataset-and-analysis-from-previous-assignment.",
    "title": "Assignment 9",
    "section": "",
    "text": "# Reading in data\ndata &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",\n                 local = locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n# Turning the Date column into a real Date\n# Dropping old Date column\ndata &lt;- data |&gt;\n  mutate(date = lubridate::dmy(Date)) |&gt;\n  select(-Date)\n\n# Turning character columns into factor variables\ndata &lt;- data |&gt;\n  mutate(seasons = factor(Seasons),\n         holiday = factor(Holiday),\n         fn_day = factor(`Functioning Day`)) |&gt;\n  select(-Seasons, -Holiday, -`Functioning Day`) # dropping old variables\n\n# Renaming other variables for simplicity\ndata &lt;- data |&gt;\n  rename(bike_count = \"Rented Bike Count\",\n    hour = \"Hour\",\n    temperature = \"Temperature(°C)\",\n    humidity = \"Humidity(%)\",\n    wind_speed = \"Wind speed (m/s)\",\n    visibility = \"Visibility (10m)\",\n    dew_point_temp = \"Dew point temperature(°C)\",\n    solar_radiation = \"Solar Radiation (MJ/m2)\",\n    rainfall = \"Rainfall(mm)\",\n    snowfall = \"Snowfall (cm)\")\n\n# Removing observations where buke rentals were out of commission.\ndata &lt;- data |&gt;\n  filter(fn_day == \"Yes\") |&gt;\n  select(-fn_day)\n\n# group_by() the date, seasons, and holiday variables and find the sum of the bike_count, rainfall, and snowfall variables and the mean of all the weather related variables.\ndata &lt;- data |&gt;\n  group_by(date, seasons, holiday) |&gt;\n  summarize(bike_count = sum(bike_count),\n             temp = mean(temperature),\n             humidity = mean(humidity),\n             wind_speed = mean(wind_speed),\n             vis = mean(visibility),\n             dew_point_temp = mean(dew_point_temp),\n             solar_radiation = mean(solar_radiation),\n             rainfall = sum(rainfall),\n             snowfall = sum(snowfall)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'date', 'seasons'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n# tidy models to split the data into a training and test set (75/25 split)\n# strata argument to stratify the split on the seasons\n\ndata_split &lt;- initial_split(data, prop = 0.75,strata = seasons)\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\n\n# 10 fold cross validation on the training set\ndata_10_fold &lt;- vfold_cv(data_train, 10)\n\n\n\n\n\n# Recipe 1\nrec_1 &lt;- recipe(bike_count ~ ., data = data_split) |&gt;\n  step_date(date, features = \"dow\") |&gt; # extracting \"dow\" column elements\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt; # removing unneeded variables\n  step_dummy(seasons, holiday, day_type) |&gt; # dummy variables for factors\n  step_normalize(all_numeric(), -bike_count) # normalizing all numeric variables besides response variable\n\n# Recipe 2\nrec_2 &lt;- rec_1 |&gt;\n  step_interact(terms = ~starts_with(\"seasons\")*starts_with(\"holiday\") +\n                  starts_with(\"seasons\")*temp +\n                  temp*rainfall) # Interactions for 2nd recipe model\n\n# Recipe 3\nrec_3 &lt;- rec_2 |&gt; \n  step_poly(temp,\n            wind_speed,\n            vis,\n            dew_point_temp,\n            solar_radiation,\n            rainfall,\n            snowfall,\n            degree = 2) # Quadratic terms for each numeric predictor\n\n\n\n\n\n# Linear regression model\nmlr_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n\n\n\n\n# Model 1\nmlr_fit_1 &lt;- workflow() |&gt;\n  add_recipe(rec_1) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  fit_resamples(data_10_fold) # 10 fold CV\n\n# Model 2\nmlr_fit_2 &lt;- workflow() |&gt;\n  add_recipe(rec_2) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  fit_resamples(data_10_fold) # 10 fold CV\n\n# Model 3\nmlr_fit_3 &lt;- workflow() |&gt;\n  add_recipe(rec_3) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  fit_resamples(data_10_fold) # 10 fold CV\n\n\n\n\n\n# Collecting model metrics for all three fitted MLR models\nrbind(mlr_fit_1 |&gt; collect_metrics(),\n      mlr_fit_2 |&gt; collect_metrics(),\n      mlr_fit_3 |&gt; collect_metrics())\n\n# A tibble: 6 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   3989.       10 228.     Preprocessor1_Model1\n2 rsq     standard      0.838    10   0.0184 Preprocessor1_Model1\n3 rmse    standard   3047.       10 219.     Preprocessor1_Model1\n4 rsq     standard      0.910    10   0.0154 Preprocessor1_Model1\n5 rmse    standard   2987.       10 214.     Preprocessor1_Model1\n6 rsq     standard      0.912    10   0.0166 Preprocessor1_Model1\n\n\nBest model is model 3: RMSE = 2784.05\n\n\n\n\nmlr_final_fit &lt;- workflow() |&gt;\n  add_recipe(rec_3) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\nmlr_final_fit |&gt;\n  collect_metrics() # Collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3088. Preprocessor1_Model1\n2 mae     standard       2342. Preprocessor1_Model1\n\n\n\n\n\n\n# Extract the final fitted parsnip model\nmlr_model &lt;- mlr_final_fit |&gt; extract_fit_parsnip()\n\n# View the model details\nmlr_model\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                        (Intercept)                             humidity  \n                            20655.3                              -2774.7  \n                     seasons_Spring                       seasons_Summer  \n                            -1687.1                               6307.7  \n                     seasons_Winter                   holiday_No.Holiday  \n                            -4822.3                                843.1  \n                   day_type_Weekend  seasons_Spring_x_holiday_No.Holiday  \n                            -1059.2                               -911.5  \nseasons_Summer_x_holiday_No.Holiday  seasons_Winter_x_holiday_No.Holiday  \n                             -571.7                               -701.9  \n              seasons_Spring_x_temp                seasons_Summer_x_temp  \n                             2240.3                              -6597.7  \n              seasons_Winter_x_temp                      temp_x_rainfall  \n                            -2406.1                              -1205.8  \n                        temp_poly_1                          temp_poly_2  \n                           -79670.6                             -10380.3  \n                  wind_speed_poly_1                    wind_speed_poly_2  \n                            -5471.0                               1713.4  \n                         vis_poly_1                           vis_poly_2  \n                             3568.1                               -128.6  \n              dew_point_temp_poly_1                dew_point_temp_poly_2  \n                           137782.2                             -14142.9  \n             solar_radiation_poly_1               solar_radiation_poly_2  \n                            45919.0                              -6091.8  \n                    rainfall_poly_1                      rainfall_poly_2  \n                           -32057.8                               8062.5  \n                    snowfall_poly_1                      snowfall_poly_2  \n                            -2194.0                              -1621.1  \n\n\n\n\n\n\ncoefficients &lt;- broom::tidy(mlr_model)\ncoefficients \n\n# A tibble: 28 × 5\n   term                                estimate std.error statistic  p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                           20655.     1700.     12.2  1.09e-26\n 2 humidity                              -2775.     1332.     -2.08 3.83e- 2\n 3 seasons_Spring                        -1687.      249.     -6.77 1.00e-10\n 4 seasons_Summer                         6308.     1018.      6.20 2.53e- 9\n 5 seasons_Winter                        -4822.     1084.     -4.45 1.35e- 5\n 6 holiday_No.Holiday                      843.      208.      4.06 6.69e- 5\n 7 day_type_Weekend                      -1059.      178.     -5.94 1.03e- 8\n 8 seasons_Spring_x_holiday_No.Holiday    -912.      291.     -3.13 1.97e- 3\n 9 seasons_Summer_x_holiday_No.Holiday    -572.      282.     -2.03 4.34e- 2\n10 seasons_Winter_x_holiday_No.Holiday    -702.      226.     -3.10 2.17e- 3\n# ℹ 18 more rows\n\n\nFor the following models (LASSO, Regression Tree, Bagged Tree, and Random Forest) we will be using recipe 1 when fitting the model workflow."
  },
  {
    "objectID": "hw9.html#fitting-a-tuned-lasso-model",
    "href": "hw9.html#fitting-a-tuned-lasso-model",
    "title": "Assignment 9",
    "section": "",
    "text": "LASSO_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\n\n\n\n\n# LASSO model 1\nLASSO_wkf &lt;- workflow() |&gt;\n  add_recipe(rec_1) |&gt; # Recipe\n  add_model(LASSO_spec) # Defined model instance\n\n\n\n\n\nLASSO_grid &lt;- LASSO_wkf |&gt;\n  tune_grid(resamples = data_10_fold,\n            grid = grid_regular(penalty(), levels = 200),\n            metrics = metric_set(rmse, mae)) # defining model metrics\n\nWarning: package 'glmnet' was built under R version 4.4.2\n\n\n\n\n\n\nLASSO_grid |&gt;\n  collect_metrics() |&gt; # collecting defining model metrics\n  filter(.metric == \"rmse\") # filter by defining model metrics\n\n# A tibble: 200 × 7\n    penalty .metric .estimator  mean     n std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   3980.    10    223. Preprocessor1_Model001\n 2 1.12e-10 rmse    standard   3980.    10    223. Preprocessor1_Model002\n 3 1.26e-10 rmse    standard   3980.    10    223. Preprocessor1_Model003\n 4 1.41e-10 rmse    standard   3980.    10    223. Preprocessor1_Model004\n 5 1.59e-10 rmse    standard   3980.    10    223. Preprocessor1_Model005\n 6 1.78e-10 rmse    standard   3980.    10    223. Preprocessor1_Model006\n 7 2.00e-10 rmse    standard   3980.    10    223. Preprocessor1_Model007\n 8 2.25e-10 rmse    standard   3980.    10    223. Preprocessor1_Model008\n 9 2.52e-10 rmse    standard   3980.    10    223. Preprocessor1_Model009\n10 2.83e-10 rmse    standard   3980.    10    223. Preprocessor1_Model010\n# ℹ 190 more rows\n\n\n\n\n\n\nlowest_rmse &lt;- LASSO_grid |&gt;\n  select_best(metric = \"rmse\")\n\n\n\n\n\nLASSO_final_fit &lt;- LASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse) |&gt;\n  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\nLASSO_final_fit |&gt;\n  collect_metrics() # collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4627. Preprocessor1_Model1\n2 mae     standard       3363. Preprocessor1_Model1\n\n\n\n\n\n\n# Extract the final model fits\nlasso_model &lt;- LASSO_final_fit |&gt;\n  extract_fit_engine()\n\n# View the final LASSO model details\nlasso_model\n\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev Lambda\n1   0  0.00 7592.0\n2   2 11.28 6918.0\n3   2 21.83 6303.0\n4   2 30.58 5743.0\n5   3 38.18 5233.0\n6   3 44.58 4768.0\n7   3 49.89 4345.0\n8   3 54.29 3959.0\n9   3 57.95 3607.0\n10  3 60.99 3286.0\n11  3 63.52 2995.0\n12  3 65.61 2729.0\n13  3 67.35 2486.0\n14  3 68.79 2265.0\n15  3 69.99 2064.0\n16  4 71.56 1881.0\n17  4 73.04 1714.0\n18  4 74.27 1561.0\n19  4 75.29 1423.0\n20  4 76.13 1296.0\n21  4 76.83 1181.0\n22  4 77.42 1076.0\n23  6 78.14  980.6\n24  6 78.83  893.5\n25  8 79.61  814.1\n26  8 80.39  741.8\n27  8 81.03  675.9\n28  8 81.57  615.8\n29  8 82.01  561.1\n30  8 82.38  511.3\n31  8 82.69  465.9\n32  8 82.94  424.5\n33  8 83.15  386.8\n34  9 83.38  352.4\n35 10 83.78  321.1\n36 10 84.11  292.6\n37 10 84.39  266.6\n38 10 84.62  242.9\n39 10 84.81  221.3\n40 10 84.97  201.7\n41 10 85.10  183.7\n42 10 85.21  167.4\n43 10 85.30  152.5\n44 10 85.37  139.0\n45 10 85.44  126.6\n46 11 85.49  115.4\n47 11 85.53  105.1\n48 11 85.57   95.8\n49 11 85.60   87.3\n50 11 85.63   79.5\n51 11 85.65   72.5\n52 11 85.67   66.0\n53 11 85.68   60.2\n54 11 85.70   54.8\n55 11 85.71   50.0\n56 11 85.72   45.5\n57 11 85.72   41.5\n58 11 85.73   37.8\n59 11 85.74   34.4\n60 12 85.74   31.4\n61 12 85.76   28.6\n62 12 85.78   26.0\n63 13 85.80   23.7\n64 13 85.81   21.6\n65 13 85.83   19.7\n66 13 85.84   18.0\n67 13 85.85   16.4\n68 13 85.85   14.9\n69 13 85.86   13.6\n70 13 85.87   12.4\n71 13 85.87   11.3\n72 13 85.87   10.3\n73 12 85.88    9.4\n74 12 85.88    8.5\n\n\n\n\n\n\nalmost_usual_fit &lt;- extract_fit_parsnip(LASSO_final_fit)\nusual_fit &lt;- almost_usual_fit$fit\nsummary(usual_fit)\n\n          Length Class     Mode   \na0         74    -none-    numeric\nbeta      962    dgCMatrix S4     \ndf         74    -none-    numeric\ndim         2    -none-    numeric\nlambda     74    -none-    numeric\ndev.ratio  74    -none-    numeric\nnulldev     1    -none-    numeric\nnpasses     1    -none-    numeric\njerr        1    -none-    numeric\noffset      1    -none-    logical\ncall        5    -none-    call   \nnobs        1    -none-    numeric"
  },
  {
    "objectID": "hw9.html#fitting-a-tuned-regression-tree-model",
    "href": "hw9.html#fitting-a-tuned-regression-tree-model",
    "title": "Assignment 9",
    "section": "",
    "text": "tree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n\n\n\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(rec_1) |&gt; # Recipe\n  add_model(tree_mod) # Defined model instance\n\n\n\n\n\ntree_fit &lt;- tree_wkf |&gt; \n  tune_grid(resamples = data_10_fold, # Cross-validation defined earlier\n            metrics = metric_set(rmse, mae)) # defining model metrics\n\n\n\n\n\ntree_fit |&gt;\n  collect_metrics() # collecting defined model metrics\n\n# A tibble: 20 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1        4.20e- 3          3 mae     standard   3519.    10    220. Preprocess…\n 2        4.20e- 3          3 rmse    standard   4463.    10    240. Preprocess…\n 3        3.89e- 6          6 mae     standard   2936.    10    191. Preprocess…\n 4        3.89e- 6          6 rmse    standard   3791.    10    248. Preprocess…\n 5        2.31e- 6          1 mae     standard   5275.    10    235. Preprocess…\n 6        2.31e- 6          1 rmse    standard   6854.    10    369. Preprocess…\n 7        2.57e- 5         11 mae     standard   2853.    10    147. Preprocess…\n 8        2.57e- 5         11 rmse    standard   3733.    10    210. Preprocess…\n 9        7.93e- 8          8 mae     standard   2859.    10    145. Preprocess…\n10        7.93e- 8          8 rmse    standard   3736.    10    205. Preprocess…\n11        8.29e-10          7 mae     standard   2834.    10    170. Preprocess…\n12        8.29e-10          7 rmse    standard   3738.    10    219. Preprocess…\n13        1.08e- 3          4 mae     standard   3396.    10    186. Preprocess…\n14        1.08e- 3          4 rmse    standard   4383.    10    234. Preprocess…\n15        4.83e- 8         10 mae     standard   2853.    10    147. Preprocess…\n16        4.83e- 8         10 rmse    standard   3733.    10    210. Preprocess…\n17        4.75e- 2         14 mae     standard   3938.    10    216. Preprocess…\n18        4.75e- 2         14 rmse    standard   4991.    10    219. Preprocess…\n19        2.71e-10         13 mae     standard   2853.    10    147. Preprocess…\n20        2.71e-10         13 rmse    standard   3733.    10    210. Preprocess…\n\n\n\n\n\n\ntree_best_params &lt;- tree_fit |&gt;\n  select_best(metric = \"rmse\") # defining the metric\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1       0.0000257         11 Preprocessor1_Model04\n\n\n\n\n\n\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\n\n\n\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\ntree_final_fit |&gt;\n  collect_metrics() # Collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4146. Preprocessor1_Model1\n2 mae     standard       3256. Preprocessor1_Model1\n\n\n\n\n\n\n# final model fits\ntree_final_model &lt;- extract_workflow(tree_final_fit) \ntree_final_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 263 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n  1) root 263 26207600000 17770.100  \n    2) temp&lt; -0.2624479 106  3126163000  8597.519  \n      4) seasons_Winter&gt;=0.561781 67   227655600  5406.627  \n        8) temp&lt; -1.542755 18    18881450  4219.556 *\n        9) temp&gt;=-1.542755 49   174092100  5842.694  \n         18) holiday_No.Holiday&lt; -1.98631 8    17059420  3759.000 *\n         19) holiday_No.Holiday&gt;=-1.98631 41   115521000  6249.268  \n           38) rainfall&gt;=-0.298057 10    19775620  4609.600 *\n           39) rainfall&lt; -0.298057 31    60187680  6778.194  \n             78) dew_point_temp&lt; -1.090833 20    38513810  6368.450  \n              156) vis&lt; 0.46653 10    12212740  5832.400 *\n              157) vis&gt;=0.46653 10    20554080  6904.500 *\n             79) dew_point_temp&gt;=-1.090833 11    12210980  7523.182 *\n      5) seasons_Winter&lt; 0.561781 39  1044377000 14079.310  \n       10) rainfall&gt;=-0.1395164 7    62808380  6631.143 *\n       11) rainfall&lt; -0.1395164 32   508295800 15708.590  \n         22) seasons_Spring&gt;=0.561781 15    81804200 12539.530 *\n         23) seasons_Spring&lt; 0.561781 17   142926100 18504.820 *\n    3) temp&gt;=-0.2624479 157  8141647000 23963.040  \n      6) solar_radiation&lt; -0.9473537 16   398061000  8440.000 *\n      7) solar_radiation&gt;=-0.9473537 141  3450651000 25724.520  \n       14) solar_radiation&lt; 0.9335255 86  2012574000 24043.870  \n         28) rainfall&gt;=-0.1488423 17   199567500 19683.290 *\n         29) rainfall&lt; -0.1488423 69  1410117000 25118.220  \n           58) temp&lt; 0.2222043 24   430172800 22407.540  \n            116) seasons_Spring&gt;=0.561781 8    77829020 17751.000 *\n            117) seasons_Spring&lt; 0.561781 16    92143210 24735.810 *\n           59) temp&gt;=0.2222043 45   709546300 26563.910  \n            118) temp&gt;=1.431878 13    67625200 23022.920 *\n            119) temp&lt; 1.431878 32   412699900 28002.440  \n              238) temp&lt; 0.4839235 9   113165800 25036.780 *\n              239) temp&gt;=0.4839235 23   189403700 29162.910  \n                478) humidity&gt;=0.2566585 12    43409210 27711.250 *\n                479) humidity&lt; 0.2566585 11    93119780 30746.550 *\n       15) solar_radiation&gt;=0.9335255 55   815330700 28352.450  \n         30) dew_point_temp&lt; 0.1597449 13    90422240 25036.690 *\n         31) dew_point_temp&gt;=0.1597449 42   537743900 29378.760  \n           62) temp&gt;=1.04259 15   133213500 25996.670 *\n           63) temp&lt; 1.04259 27   137630500 31257.700  \n            126) dew_point_temp&lt; 0.5312419 9    16952300 28869.110 *\n            127) dew_point_temp&gt;=0.5312419 18    43655670 32452.000 *\n\n\n\n\n\n\n# Extract the fitted engine \ntree_final_fit_engine &lt;- tree_final_model |&gt; extract_fit_engine()\n\n# Plot the decision tree using rpart.plot\nrpart.plot(tree_final_fit_engine, roundint = FALSE)"
  },
  {
    "objectID": "hw9.html#fitting-a-tuned-bagged-tree-model",
    "href": "hw9.html#fitting-a-tuned-bagged-tree-model",
    "title": "Assignment 9",
    "section": "",
    "text": "bag_spec &lt;- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |&gt;\n set_engine(\"rpart\") |&gt;\n set_mode(\"regression\")\n\n\n\n\n\nbag_wkf &lt;- workflow() |&gt;\n add_recipe(rec_1) |&gt; # Recipe\n add_model(bag_spec) # Defined model instance\n\n\n\n\n\nbag_fit &lt;- bag_wkf |&gt;\n tune_grid(resamples = data_10_fold, \n           grid = grid_regular(cost_complexity(), \n                               levels = 15), \n           metrics = metric_set(rmse, mae)) # defining model metrics\n\n\n\n\n\nbag_fit |&gt;\n collect_metrics() |&gt; # collecting defined model metrics\n filter(.metric == \"rmse\") |&gt;\n arrange(mean) # Arranged by mean RMSE value\n\n# A tibble: 15 × 7\n   cost_complexity .metric .estimator  mean     n std_err .config              \n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1        1.64e- 7 rmse    standard   3161.    10    155. Preprocessor1_Model06\n 2        2.68e- 4 rmse    standard   3186.    10    182. Preprocessor1_Model11\n 3        1.18e- 3 rmse    standard   3212.    10    175. Preprocessor1_Model12\n 4        6.11e- 5 rmse    standard   3216.    10    165. Preprocessor1_Model10\n 5        8.48e- 9 rmse    standard   3220.    10    181. Preprocessor1_Model04\n 6        4.39e-10 rmse    standard   3229.    10    136. Preprocessor1_Model02\n 7        3.16e- 6 rmse    standard   3247.    10    183. Preprocessor1_Model08\n 8        1.93e- 9 rmse    standard   3286.    10    209. Preprocessor1_Model03\n 9        1.39e- 5 rmse    standard   3289.    10    152. Preprocessor1_Model09\n10        1   e-10 rmse    standard   3295.    10    198. Preprocessor1_Model01\n11        7.20e- 7 rmse    standard   3308.    10    220. Preprocessor1_Model07\n12        3.73e- 8 rmse    standard   3312.    10    248. Preprocessor1_Model05\n13        5.18e- 3 rmse    standard   3420.    10    209. Preprocessor1_Model13\n14        2.28e- 2 rmse    standard   3815.    10    159. Preprocessor1_Model14\n15        1   e- 1 rmse    standard   4575.    10    155. Preprocessor1_Model15\n\n\n\n\n\n\nbag_best_params &lt;- bag_fit |&gt;\n  select_best(metric = \"rmse\")\nbag_best_params\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1     0.000000164 Preprocessor1_Model06\n\n\n\n\n\n\nbag_final_wkf &lt;- bag_wkf |&gt;\n finalize_workflow(bag_best_params)\n\n\n\n\n\nbag_final_fit &lt;- bag_final_wkf |&gt;\n last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\nbag_final_fit |&gt;\n  collect_metrics() # Collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3433. Preprocessor1_Model1\n2 mae     standard       2580. Preprocessor1_Model1\n\n\n\n\n\n\n# final model fits\nbag_final_model &lt;- extract_fit_engine(bag_final_fit) \nbag_final_model\n\nBagged CART (regression with 11 members)\n\nVariable importance scores include:\n\n# A tibble: 13 × 4\n   term                      value  std.error  used\n   &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n 1 temp               16870562538. 817360583.    11\n 2 dew_point_temp     13339222095. 900377817.    11\n 3 solar_radiation    12089267018. 530510065.    11\n 4 seasons_Winter     10960104116. 655774213.    11\n 5 humidity            5763991151. 563552149.    11\n 6 rainfall            3345702917. 287806910.    11\n 7 snowfall            2991482977. 514851687.    11\n 8 wind_speed          1597564355. 271651017.    11\n 9 seasons_Summer      1201710287. 583650229.     8\n10 seasons_Spring       910799554. 156589814.    11\n11 vis                  797546421.  90844925.    11\n12 day_type_Weekend     205077700.  52658416.    10\n13 holiday_No.Holiday   100978457.  57953074.     9\n\n\n\n\n\n\n# Creating variable importance plot\nbag_final_model$imp |&gt;\n mutate(term = factor(term, levels = term)) |&gt;\n ggplot(aes(x = term, y = value)) +\n geom_bar(stat =\"identity\") +\n coord_flip() +\n  labs(title = \"Variable Importance for Bagged Tree Model\",\n       x = \"Features\",\n       y = \"Importance\")"
  },
  {
    "objectID": "hw9.html#fitting-a-tuned-random-forest-model",
    "href": "hw9.html#fitting-a-tuned-random-forest-model",
    "title": "Assignment 9",
    "section": "",
    "text": "rf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n set_engine(\"ranger\", importance = \"impurity\") |&gt;\n set_mode(\"regression\")\n\n\n\n\n\nrf_wkf &lt;- workflow() |&gt;\n add_recipe(rec_1) |&gt; # Recipe\n add_model(rf_spec) # Defined model instance\n\n\n\n\n\nrf_fit &lt;- rf_wkf |&gt;\n tune_grid(resamples = data_10_fold,\n grid = 7,\n metrics = metric_set(rmse, mae)) # defining model metrics\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n\n\n\n\nrf_fit |&gt;\n collect_metrics() |&gt; # Collecting defined model metrics\n filter(.metric == \"rmse\") |&gt;\n arrange(mean) # Arranged by mean RMSE value\n\n# A tibble: 7 × 7\n   mtry .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     9 rmse    standard   2833.    10    148. Preprocessor1_Model3\n2    11 rmse    standard   2850.    10    134. Preprocessor1_Model4\n3     8 rmse    standard   2865.    10    140. Preprocessor1_Model5\n4    13 rmse    standard   2877.    10    129. Preprocessor1_Model7\n5     6 rmse    standard   2880.    10    159. Preprocessor1_Model1\n6     4 rmse    standard   2982.    10    182. Preprocessor1_Model2\n7     2 rmse    standard   3355.    10    204. Preprocessor1_Model6\n\n\n\n\n\n\nrf_best_params &lt;- rf_fit |&gt;\n  select_best(metric = \"rmse\") # defining the metric\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     9 Preprocessor1_Model3\n\n\n\n\n\n\nrf_final_wkf &lt;- rf_wkf |&gt;\n finalize_workflow(rf_best_params)\n\n\n\n\n\nrf_final_fit &lt;- rf_final_wkf |&gt;\n last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\nrf_final_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [263/90]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\nrf_final_fit |&gt;\n  collect_metrics() # Collecting by defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3128. Preprocessor1_Model1\n2 mae     standard       2420. Preprocessor1_Model1\n\n\n\n\n\n\n# final model fits\nrf_final_model &lt;- extract_fit_engine(rf_final_fit) \nrf_final_model\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~9L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      263 \nNumber of independent variables:  13 \nMtry:                             9 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       8006527 \nR squared (OOB):                  0.919958 \n\n\n\n\n\n\n# Extract variable importance and convert to a tibble\nrf_imp &lt;- rf_final_model$variable.importance\nrf_imp_df &lt;- tibble(\n  term = names(rf_imp), # Feature names\n  value = rf_imp # Feature importance values\n)\n\n# Creating variable importance plot\nrf_imp_df |&gt;\n  mutate(term = factor(term, levels = term)) |&gt;\n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Variable Importance for Random Forest Model\",\n       x = \"Features\",\n       y = \"Importance\")"
  },
  {
    "objectID": "hw9.html#comparing-all-final-models-using-root-mean-squared-error-rmse-and-mean-absolute-error-mae",
    "href": "hw9.html#comparing-all-final-models-using-root-mean-squared-error-rmse-and-mean-absolute-error-mae",
    "title": "Assignment 9",
    "section": "",
    "text": "# MLR model \nmlr_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3088. Preprocessor1_Model1\n2 mae     standard       2342. Preprocessor1_Model1\n\n\n\n# tuned LASSO Model\nLASSO_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4627. Preprocessor1_Model1\n2 mae     standard       3363. Preprocessor1_Model1\n\n\n\n# tuned Regression Tree Model\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4146. Preprocessor1_Model1\n2 mae     standard       3256. Preprocessor1_Model1\n\n\n\n# tuned Bagged Tree Model\nbag_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3433. Preprocessor1_Model1\n2 mae     standard       2580. Preprocessor1_Model1\n\n\n\n# tuned Random Forest Model\nrf_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3128. Preprocessor1_Model1\n2 mae     standard       2420. Preprocessor1_Model1\n\n\nIn comparison of all final models: the tuned Random Forest model is the best!"
  },
  {
    "objectID": "hw9.html#fitting-the-best-overall-model-to-the-entire-dataset",
    "href": "hw9.html#fitting-the-best-overall-model-to-the-entire-dataset",
    "title": "Assignment 9",
    "section": "",
    "text": "# Fit the model to the entire data set! \nrf_final_fit &lt;- rf_final_wkf |&gt;\n  fit(data)\n\n# Print the final fitted model\nrf_final_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~9L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      353 \nNumber of independent variables:  13 \nMtry:                             9 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       7634549 \nR squared (OOB):                  0.922686"
  }
]