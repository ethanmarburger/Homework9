[
  {
    "objectID": "hw9.html",
    "href": "hw9.html",
    "title": "Assignment 9",
    "section": "",
    "text": "# Reading in data\ndata &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",\n                 local = locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n# Turning the Date column into a real Date\n# Dropping old Date column\ndata &lt;- data |&gt;\n  mutate(date = lubridate::dmy(Date)) |&gt;\n  select(-Date)\n\n# Turning character columns into factor variables\ndata &lt;- data |&gt;\n  mutate(seasons = factor(Seasons),\n         holiday = factor(Holiday),\n         fn_day = factor(`Functioning Day`)) |&gt;\n  select(-Seasons, -Holiday, -`Functioning Day`) # dropping old variables\n\n# Renaming other variables for simplicity\ndata &lt;- data |&gt;\n  rename(bike_count = \"Rented Bike Count\",\n    hour = \"Hour\",\n    temperature = \"Temperature(°C)\",\n    humidity = \"Humidity(%)\",\n    wind_speed = \"Wind speed (m/s)\",\n    visibility = \"Visibility (10m)\",\n    dew_point_temp = \"Dew point temperature(°C)\",\n    solar_radiation = \"Solar Radiation (MJ/m2)\",\n    rainfall = \"Rainfall(mm)\",\n    snowfall = \"Snowfall (cm)\")\n\n# Removing observations where buke rentals were out of commission.\ndata &lt;- data |&gt;\n  filter(fn_day == \"Yes\") |&gt;\n  select(-fn_day)\n\n# group_by() the date, seasons, and holiday variables and find the sum of the bike_count, rainfall, and snowfall variables and the mean of all the weather related variables.\ndata &lt;- data |&gt;\n  group_by(date, seasons, holiday) |&gt;\n  summarize(bike_count = sum(bike_count),\n             temp = mean(temperature),\n             humidity = mean(humidity),\n             wind_speed = mean(wind_speed),\n             vis = mean(visibility),\n             dew_point_temp = mean(dew_point_temp),\n             solar_radiation = mean(solar_radiation),\n             rainfall = sum(rainfall),\n             snowfall = sum(snowfall)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'date', 'seasons'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n# tidy models to split the data into a training and test set (75/25 split)\n# strata argument to stratify the split on the seasons\n\ndata_split &lt;- initial_split(data, prop = 0.75,strata = seasons)\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\n\n# 10 fold cross validation on the training set\ndata_10_fold &lt;- vfold_cv(data_train, 10)\n\n\n\n\n\n# Recipe 1\nrec_1 &lt;- recipe(bike_count ~ ., data = data_split) |&gt;\n  step_date(date, features = \"dow\") |&gt; # extracting \"dow\" column elements\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt; # removing unneeded variables\n  step_dummy(seasons, holiday, day_type) |&gt; # dummy variables for factors\n  step_normalize(all_numeric(), -bike_count) # normalizing all numeric variables besides response variable\n\n# Recipe 2\nrec_2 &lt;- rec_1 |&gt;\n  step_interact(terms = ~starts_with(\"seasons\")*starts_with(\"holiday\") +\n                  starts_with(\"seasons\")*temp +\n                  temp*rainfall) # Interactions for 2nd recipe model\n\n# Recipe 3\nrec_3 &lt;- rec_2 |&gt; \n  step_poly(temp,\n            wind_speed,\n            vis,\n            dew_point_temp,\n            solar_radiation,\n            rainfall,\n            snowfall,\n            degree = 2) # Quadratic terms for each numeric predictor\n\n\n\n\n\n# Linear regression model\nmlr_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n\n\n\n\n# Model 1\nmlr_fit_1 &lt;- workflow() |&gt;\n  add_recipe(rec_1) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  fit_resamples(data_10_fold) # 10 fold CV\n\n# Model 2\nmlr_fit_2 &lt;- workflow() |&gt;\n  add_recipe(rec_2) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  fit_resamples(data_10_fold) # 10 fold CV\n\n# Model 3\nmlr_fit_3 &lt;- workflow() |&gt;\n  add_recipe(rec_3) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  fit_resamples(data_10_fold) # 10 fold CV\n\n\n\n\n\n# Collecting model metrics for all three fitted MLR models\nrbind(mlr_fit_1 |&gt; collect_metrics(),\n      mlr_fit_2 |&gt; collect_metrics(),\n      mlr_fit_3 |&gt; collect_metrics())\n\n# A tibble: 6 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   3969.       10 185.     Preprocessor1_Model1\n2 rsq     standard      0.852    10   0.0125 Preprocessor1_Model1\n3 rmse    standard   3182.       10 197.     Preprocessor1_Model1\n4 rsq     standard      0.904    10   0.0114 Preprocessor1_Model1\n5 rmse    standard   3114.       10 200.     Preprocessor1_Model1\n6 rsq     standard      0.908    10   0.0115 Preprocessor1_Model1\n\n\nBest model is model 3\n\n\n\n\nmlr_final_fit &lt;- workflow() |&gt;\n  add_recipe(rec_3) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\nmlr_final_fit |&gt;\n  collect_metrics() # Collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2770. Preprocessor1_Model1\n2 mae     standard       2224. Preprocessor1_Model1\n\n\n\n\n\n\n# Extract the final fitted parsnip model\nmlr_model &lt;- mlr_final_fit |&gt; extract_fit_parsnip()\n\n# View the model details\nmlr_model\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                        (Intercept)                             humidity  \n                            19906.7                              -1094.6  \n                     seasons_Spring                       seasons_Summer  \n                            -1856.4                               5649.9  \n                     seasons_Winter                   holiday_No.Holiday  \n                            -4817.2                                768.5  \n                   day_type_Weekend  seasons_Spring_x_holiday_No.Holiday  \n                            -1062.8                               -295.8  \nseasons_Summer_x_holiday_No.Holiday  seasons_Winter_x_holiday_No.Holiday  \n                             -214.0                               -362.8  \n              seasons_Spring_x_temp                seasons_Summer_x_temp  \n                             2245.9                              -6087.2  \n              seasons_Winter_x_temp                      temp_x_rainfall  \n                            -2129.0                               -621.5  \n                        temp_poly_1                          temp_poly_2  \n                           -17366.7                             -16127.7  \n                  wind_speed_poly_1                    wind_speed_poly_2  \n                            -4600.1                               1594.5  \n                         vis_poly_1                           vis_poly_2  \n                             5474.9                              -1313.6  \n              dew_point_temp_poly_1                dew_point_temp_poly_2  \n                            64361.6                              -5017.9  \n             solar_radiation_poly_1               solar_radiation_poly_2  \n                            48127.0                              -4002.4  \n                    rainfall_poly_1                      rainfall_poly_2  \n                           -42702.7                              13333.2  \n                    snowfall_poly_1                      snowfall_poly_2  \n                            -2682.3                               -336.9  \n\n\n\n\n\n\ncoefficients &lt;- broom::tidy(mlr_model)\ncoefficients \n\n# A tibble: 28 × 5\n   term                                estimate std.error statistic  p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                           19907.     1571.    12.7   2.17e-28\n 2 humidity                              -1095.     1370.    -0.799 4.25e- 1\n 3 seasons_Spring                        -1856.      260.    -7.15  1.10e-11\n 4 seasons_Summer                         5650.     1012.     5.58  6.59e- 8\n 5 seasons_Winter                        -4817.     1003.    -4.80  2.81e- 6\n 6 holiday_No.Holiday                      768.      201.     3.83  1.67e- 4\n 7 day_type_Weekend                      -1063.      185.    -5.74  2.89e- 8\n 8 seasons_Spring_x_holiday_No.Holiday    -296.      249.    -1.19  2.36e- 1\n 9 seasons_Summer_x_holiday_No.Holiday    -214.      269.    -0.795 4.27e- 1\n10 seasons_Winter_x_holiday_No.Holiday    -363.      201.    -1.80  7.25e- 2\n# ℹ 18 more rows\n\n\nFor the following models (LASSO, Regression Tree, Bagged Tree, and Random Forest) we will be using recipe 1 when fitting the model workflow.\n\n\n\n\n\n\n\nLASSO_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\n\n\n\n\n# LASSO model 1\nLASSO_wkf &lt;- workflow() |&gt;\n  add_recipe(rec_1) |&gt; # Recipe\n  add_model(LASSO_spec) # Defined model instance\n\n\n\n\n\nLASSO_grid &lt;- LASSO_wkf |&gt;\n  tune_grid(resamples = data_10_fold,\n            grid = grid_regular(penalty(), levels = 200),\n            metrics = metric_set(rmse, mae)) # defining model metrics\n\nWarning: package 'glmnet' was built under R version 4.4.2\n\n\n\n\n\n\nLASSO_grid |&gt;\n  collect_metrics() |&gt; # collecting defining model metrics\n  filter(.metric == \"rmse\") # filter by defining model metrics\n\n# A tibble: 200 × 7\n    penalty .metric .estimator  mean     n std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   3960.    10    189. Preprocessor1_Model001\n 2 1.12e-10 rmse    standard   3960.    10    189. Preprocessor1_Model002\n 3 1.26e-10 rmse    standard   3960.    10    189. Preprocessor1_Model003\n 4 1.41e-10 rmse    standard   3960.    10    189. Preprocessor1_Model004\n 5 1.59e-10 rmse    standard   3960.    10    189. Preprocessor1_Model005\n 6 1.78e-10 rmse    standard   3960.    10    189. Preprocessor1_Model006\n 7 2.00e-10 rmse    standard   3960.    10    189. Preprocessor1_Model007\n 8 2.25e-10 rmse    standard   3960.    10    189. Preprocessor1_Model008\n 9 2.52e-10 rmse    standard   3960.    10    189. Preprocessor1_Model009\n10 2.83e-10 rmse    standard   3960.    10    189. Preprocessor1_Model010\n# ℹ 190 more rows\n\n\n\n\n\n\nlowest_rmse &lt;- LASSO_grid |&gt;\n  select_best(metric = \"rmse\")\n\n\n\n\n\nLASSO_final_fit &lt;- LASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse) |&gt;\n  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\nLASSO_final_fit |&gt;\n  collect_metrics() # collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4547. Preprocessor1_Model1\n2 mae     standard       3528. Preprocessor1_Model1\n\n\n\n\n\n\n# Extract the final model fits\nlasso_model &lt;- LASSO_final_fit |&gt;\n  extract_fit_engine()\n\n# View the final LASSO model details\nlasso_model\n\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev Lambda\n1   0  0.00 7266.0\n2   2 12.01 6621.0\n3   2 22.33 6032.0\n4   2 30.89 5497.0\n5   3 38.17 5008.0\n6   3 44.46 4563.0\n7   3 49.68 4158.0\n8   3 54.02 3789.0\n9   3 57.62 3452.0\n10  3 60.61 3145.0\n11  3 63.09 2866.0\n12  3 65.14 2611.0\n13  3 66.85 2379.0\n14  3 68.27 2168.0\n15  3 69.45 1975.0\n16  3 70.43 1800.0\n17  4 71.90 1640.0\n18  4 73.16 1494.0\n19  4 74.20 1362.0\n20  4 75.07 1241.0\n21  4 75.79 1130.0\n22  5 76.42 1030.0\n23  5 77.29  938.5\n24  5 78.01  855.1\n25  7 78.73  779.1\n26  7 79.41  709.9\n27  7 79.98  646.8\n28  7 80.45  589.4\n29  8 80.89  537.0\n30  8 81.26  489.3\n31  8 81.57  445.8\n32  8 81.83  406.2\n33  9 82.14  370.1\n34 10 82.59  337.3\n35 10 82.97  307.3\n36 10 83.28  280.0\n37 10 83.54  255.1\n38 10 83.76  232.5\n39 10 83.94  211.8\n40 10 84.09  193.0\n41 10 84.21  175.8\n42 10 84.32  160.2\n43 10 84.40  146.0\n44 10 84.47  133.0\n45 10 84.53  121.2\n46 10 84.58  110.4\n47 10 84.62  100.6\n48 11 84.65   91.7\n49 11 84.68   83.5\n50 11 84.71   76.1\n51 11 84.73   69.4\n52 11 84.74   63.2\n53 12 84.76   57.6\n54 12 84.77   52.5\n55 12 84.78   47.8\n56 11 84.78   43.6\n57 11 84.79   39.7\n58 12 84.80   36.2\n59 12 84.80   33.0\n60 12 84.82   30.0\n61 12 84.84   27.4\n62 12 84.86   24.9\n63 12 84.87   22.7\n64 12 84.88   20.7\n65 12 84.89   18.9\n66 12 84.90   17.2\n67 12 84.91   15.7\n68 12 84.91   14.3\n69 13 84.92   13.0\n70 13 84.93   11.8\n71 13 84.93   10.8\n72 13 84.93    9.8\n73 13 84.94    9.0\n74 13 84.94    8.2\n75 13 84.94    7.4\n76 13 84.94    6.8\n77 12 84.95    6.2\n78 12 84.95    5.6\n\n\n\n\n\n\nalmost_usual_fit &lt;- extract_fit_parsnip(LASSO_final_fit)\nusual_fit &lt;- almost_usual_fit$fit\nsummary(usual_fit)\n\n          Length Class     Mode   \na0          78   -none-    numeric\nbeta      1014   dgCMatrix S4     \ndf          78   -none-    numeric\ndim          2   -none-    numeric\nlambda      78   -none-    numeric\ndev.ratio   78   -none-    numeric\nnulldev      1   -none-    numeric\nnpasses      1   -none-    numeric\njerr         1   -none-    numeric\noffset       1   -none-    logical\ncall         5   -none-    call   \nnobs         1   -none-    numeric\n\n\n\n\n\n\n\n\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n\n\n\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(rec_1) |&gt; # Recipe\n  add_model(tree_mod) # Defined model instance\n\n\n\n\n\ntree_fit &lt;- tree_wkf |&gt; \n  tune_grid(resamples = data_10_fold, # Cross-validation defined earlier\n            metrics = metric_set(rmse, mae)) # defining model metrics\n\n\n\n\n\ntree_fit |&gt;\n  collect_metrics() # collecting defined model metrics\n\n# A tibble: 20 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1        6.52e-10         11 mae     standard   2646.    10    111. Preprocess…\n 2        6.52e-10         11 rmse    standard   3575.    10    174. Preprocess…\n 3        3.42e- 9          5 mae     standard   2817.    10    125. Preprocess…\n 4        3.42e- 9          5 rmse    standard   3782.    10    179. Preprocess…\n 5        8.73e- 8          7 mae     standard   2667.    10    117. Preprocess…\n 6        8.73e- 8          7 rmse    standard   3594.    10    176. Preprocess…\n 7        1.76e- 6         11 mae     standard   2646.    10    111. Preprocess…\n 8        1.76e- 6         11 rmse    standard   3575.    10    174. Preprocess…\n 9        6.02e- 2          2 mae     standard   3524.    10    226. Preprocess…\n10        6.02e- 2          2 rmse    standard   4477.    10    231. Preprocess…\n11        3.15e- 5          2 mae     standard   3445.    10    210. Preprocess…\n12        3.15e- 5          2 rmse    standard   4400.    10    223. Preprocess…\n13        6.87e- 3          9 mae     standard   2882.    10    140. Preprocess…\n14        6.87e- 3          9 rmse    standard   3782.    10    188. Preprocess…\n15        1.74e- 5         13 mae     standard   2646.    10    111. Preprocess…\n16        1.74e- 5         13 rmse    standard   3575.    10    174. Preprocess…\n17        3.16e- 4         14 mae     standard   2646.    10    111. Preprocess…\n18        3.16e- 4         14 rmse    standard   3575.    10    174. Preprocess…\n19        8.11e- 9          5 mae     standard   2817.    10    125. Preprocess…\n20        8.11e- 9          5 rmse    standard   3782.    10    179. Preprocess…\n\n\n\n\n\n\ntree_best_params &lt;- tree_fit |&gt;\n  select_best(metric = \"rmse\") # defining the metric\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1        0.000316         14 Preprocessor1_Model09\n\n\n\n\n\n\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\n\n\n\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\ntree_final_fit |&gt;\n  collect_metrics() # Collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3799. Preprocessor1_Model1\n2 mae     standard       2966. Preprocessor1_Model1\n\n\n\n\n\n\n# final model fits\ntree_final_model &lt;- extract_workflow(tree_final_fit) \ntree_final_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 263 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n  1) root 263 24958860000 17099.720  \n    2) temp&lt; -0.2201567 113  3337065000  8759.195  \n      4) seasons_Winter&gt;=0.561781 67   209102200  5357.239  \n        8) temp&lt; -1.472334 21    23429830  4127.286 *\n        9) temp&gt;=-1.472334 46   139400900  5918.739  \n         18) holiday_No.Holiday&lt; -1.833776 7    16589840  3850.571 *\n         19) holiday_No.Holiday&gt;=-1.833776 39    87495740  6289.949  \n           38) snowfall&gt;=0.05246307 9    11765280  4952.778 *\n           39) snowfall&lt; 0.05246307 30    54810550  6691.100  \n             78) day_type_Weekend&gt;=0.4151317 11    14333350  5637.455 *\n             79) day_type_Weekend&lt; 0.4151317 19    21195320  7301.105 *\n      5) seasons_Winter&lt; 0.561781 46  1223148000 13714.220  \n       10) seasons_Spring&gt;=0.561781 24   292468100 10312.880  \n         20) rainfall&gt;=-0.2414439 8    52772870  6782.625 *\n         21) rainfall&lt; -0.2414439 16    90143270 12078.000 *\n       11) seasons_Spring&lt; 0.561781 22   350119700 17424.770  \n         22) solar_radiation&lt; -0.7186281 7   101498900 13361.000 *\n         23) solar_radiation&gt;=-0.7186281 15    79074510 19321.200 *\n    3) temp&gt;=-0.2201567 150  7839229000 23382.920  \n      6) solar_radiation&lt; -1.058026 15   231919400  7188.933 *\n      7) solar_radiation&gt;=-1.058026 135  3236556000 25182.250  \n       14) solar_radiation&lt; 0.1840252 39   700988100 21654.920  \n         28) solar_radiation&lt; -0.5000219 13   138297700 18620.540 *\n         29) solar_radiation&gt;=-0.5000219 26   383144400 23172.120  \n           58) humidity&lt; 0.7966163 19   247086800 22028.790 *\n           59) humidity&gt;=0.7966163 7    43806990 26275.430 *\n       15) solar_radiation&gt;=0.1840252 96  1853199000 26615.230  \n         30) temp&gt;=1.501809 15    80731950 21476.330 *\n         31) temp&lt; 1.501809 81  1302987000 27566.880  \n           62) temp&lt; 0.1956372 14   173282400 22392.710 *\n           63) temp&gt;=0.1956372 67   676579600 28648.040  \n            126) temp&lt; 0.5094808 18   193311100 26427.280 *\n            127) temp&gt;=0.5094808 49   361885700 29463.840  \n              254) temp&gt;=1.021364 18    48372290 27055.280 *\n              255) temp&lt; 1.021364 31   148461100 30862.350  \n                510) seasons_Summer&lt; 0.5391721 17    37981820 30034.000 *\n                511) seasons_Summer&gt;=0.5391721 14    84649850 31868.210 *\n\n\n\n\n\n\n# Extract the fitted engine \ntree_final_fit_engine &lt;- tree_final_model |&gt; extract_fit_engine()\n\n# Plot the decision tree using rpart.plot\nrpart.plot(tree_final_fit_engine, roundint = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbag_spec &lt;- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |&gt;\n set_engine(\"rpart\") |&gt;\n set_mode(\"regression\")\n\n\n\n\n\nbag_wkf &lt;- workflow() |&gt;\n add_recipe(rec_1) |&gt; # Recipe\n add_model(bag_spec) # Defined model instance\n\n\n\n\n\nbag_fit &lt;- bag_wkf |&gt;\n tune_grid(resamples = data_10_fold, \n           grid = grid_regular(cost_complexity(), \n                               levels = 15), \n           metrics = metric_set(rmse, mae)) # defining model metrics\n\n\n\n\n\nbag_fit |&gt;\n collect_metrics() |&gt; # collecting defined model metrics\n filter(.metric == \"rmse\") |&gt;\n arrange(mean) # Arranged by mean RMSE value\n\n# A tibble: 15 × 7\n   cost_complexity .metric .estimator  mean     n std_err .config              \n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1        1.93e- 9 rmse    standard   3115.    10    169. Preprocessor1_Model03\n 2        7.20e- 7 rmse    standard   3162.    10    153. Preprocessor1_Model07\n 3        2.68e- 4 rmse    standard   3164.    10    144. Preprocessor1_Model11\n 4        3.16e- 6 rmse    standard   3201.    10    157. Preprocessor1_Model08\n 5        1.64e- 7 rmse    standard   3204.    10    132. Preprocessor1_Model06\n 6        4.39e-10 rmse    standard   3224.    10    156. Preprocessor1_Model02\n 7        8.48e- 9 rmse    standard   3229.    10    159. Preprocessor1_Model04\n 8        1.18e- 3 rmse    standard   3233.    10    175. Preprocessor1_Model12\n 9        6.11e- 5 rmse    standard   3240.    10    152. Preprocessor1_Model10\n10        1.39e- 5 rmse    standard   3245.    10    115. Preprocessor1_Model09\n11        1   e-10 rmse    standard   3251.    10    118. Preprocessor1_Model01\n12        3.73e- 8 rmse    standard   3258.    10    166. Preprocessor1_Model05\n13        5.18e- 3 rmse    standard   3347.    10    140. Preprocessor1_Model13\n14        2.28e- 2 rmse    standard   3805.    10    184. Preprocessor1_Model14\n15        1   e- 1 rmse    standard   4807.    10    195. Preprocessor1_Model15\n\n\n\n\n\n\nbag_best_params &lt;- bag_fit |&gt;\n  select_best(metric = \"rmse\")\nbag_best_params\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1   0.00000000193 Preprocessor1_Model03\n\n\n\n\n\n\nbag_final_wkf &lt;- bag_wkf |&gt;\n finalize_workflow(bag_best_params)\n\n\n\n\n\nbag_final_fit &lt;- bag_final_wkf |&gt;\n last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\nbag_final_fit |&gt;\n  collect_metrics() # Collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3692. Preprocessor1_Model1\n2 mae     standard       3022. Preprocessor1_Model1\n\n\n\n\n\n\n# final model fits\nbag_final_model &lt;- extract_fit_engine(bag_final_fit) \nbag_final_model\n\nBagged CART (regression with 11 members)\n\nVariable importance scores include:\n\n# A tibble: 13 × 4\n   term                      value  std.error  used\n   &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n 1 temp               15443136673. 454365045.    11\n 2 solar_radiation    12633999997. 496795895.    11\n 3 dew_point_temp     12426918314. 589638733.    11\n 4 seasons_Winter      9449409307. 518154069.    11\n 5 humidity            7510558669. 505634340.    11\n 6 rainfall            4021505847. 274143212.    11\n 7 wind_speed          2182743273. 448114758.    11\n 8 seasons_Summer      1601058133. 644754532.     8\n 9 vis                 1584063189. 161475765.    11\n10 snowfall            1210680245. 442348352.    10\n11 seasons_Spring      1192847994. 166328461.    11\n12 day_type_Weekend     246104025. 177253467.    11\n13 holiday_No.Holiday    44282323.  29462086.     6\n\n\n\n\n\n\n# Creating variable importance plot\nbag_final_model$imp |&gt;\n mutate(term = factor(term, levels = term)) |&gt;\n ggplot(aes(x = term, y = value)) +\n geom_bar(stat =\"identity\") +\n coord_flip() +\n  labs(title = \"Variable Importance for Bagged Tree Model\",\n       x = \"Features\",\n       y = \"Importance\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n set_engine(\"ranger\", importance = \"impurity\") |&gt;\n set_mode(\"regression\")\n\n\n\n\n\nrf_wkf &lt;- workflow() |&gt;\n add_recipe(rec_1) |&gt; # Recipe\n add_model(rf_spec) # Defined model instance\n\n\n\n\n\nrf_fit &lt;- rf_wkf |&gt;\n tune_grid(resamples = data_10_fold,\n grid = 7,\n metrics = metric_set(rmse, mae)) # defining model metrics\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n\n\n\n\nrf_fit |&gt;\n collect_metrics() |&gt; # Collecting defined model metrics\n filter(.metric == \"rmse\") |&gt;\n arrange(mean) # Arranged by mean RMSE value\n\n# A tibble: 7 × 7\n   mtry .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1    13 rmse    standard   2879.    10    124. Preprocessor1_Model6\n2    11 rmse    standard   2897.    10    115. Preprocessor1_Model2\n3     9 rmse    standard   2911.    10    115. Preprocessor1_Model3\n4     7 rmse    standard   2950.    10    123. Preprocessor1_Model5\n5     6 rmse    standard   2989.    10    123. Preprocessor1_Model1\n6     3 rmse    standard   3237.    10    134. Preprocessor1_Model4\n7     2 rmse    standard   3503.    10    152. Preprocessor1_Model7\n\n\n\n\n\n\nrf_best_params &lt;- rf_fit |&gt;\n  select_best(metric = \"rmse\") # defining the metric\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1    13 Preprocessor1_Model6\n\n\n\n\n\n\nrf_final_wkf &lt;- rf_wkf |&gt;\n finalize_workflow(rf_best_params)\n\n\n\n\n\nrf_final_fit &lt;- rf_final_wkf |&gt;\n last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\nrf_final_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [263/90]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\nrf_final_fit |&gt;\n  collect_metrics() # Collecting by defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2859. Preprocessor1_Model1\n2 mae     standard       2348. Preprocessor1_Model1\n\n\n\n\n\n\n# final model fits\nrf_final_model &lt;- extract_fit_engine(rf_final_fit) \nrf_final_model\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~13L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      263 \nNumber of independent variables:  13 \nMtry:                             13 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       8174479 \nR squared (OOB):                  0.9141902 \n\n\n\n\n\n\n# Extract variable importance and convert to a tibble\nrf_imp &lt;- rf_final_model$variable.importance\nrf_imp_df &lt;- tibble(\n  term = names(rf_imp), # Feature names\n  value = rf_imp # Feature importance values\n)\n\n# Creating variable importance plot\nrf_imp_df |&gt;\n  mutate(term = factor(term, levels = term)) |&gt;\n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Variable Importance for Random Forest Model\",\n       x = \"Features\",\n       y = \"Importance\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n# MLR model \nmlr_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2770. Preprocessor1_Model1\n2 mae     standard       2224. Preprocessor1_Model1\n\n\n\n# tuned LASSO Model\nLASSO_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4547. Preprocessor1_Model1\n2 mae     standard       3528. Preprocessor1_Model1\n\n\n\n# tuned Regression Tree Model\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3799. Preprocessor1_Model1\n2 mae     standard       2966. Preprocessor1_Model1\n\n\n\n# tuned Bagged Tree Model\nbag_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3692. Preprocessor1_Model1\n2 mae     standard       3022. Preprocessor1_Model1\n\n\n\n# tuned Random Forest Model\nrf_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2859. Preprocessor1_Model1\n2 mae     standard       2348. Preprocessor1_Model1\n\n\nIn comparison of all final models: the Multiple Linear Model is the best!\nFYI: I didn’t set a seed so every time I render my quarto document the best final model changes between the MLR and Random Forest models. Most of the time, the MLR model is the best.\n\n\n\n\n# Fit the model to the entire dataset\nMLR_final &lt;- workflow() |&gt;\n  add_recipe(rec_3) |&gt; # Recipe\n  add_model(mlr_spec) |&gt;\n  fit(data)\n\nMLR_final |&gt;\n  tidy()\n\n# A tibble: 28 × 5\n   term                                estimate std.error statistic  p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                           20922.     1288.    16.2   7.52e-44\n 2 humidity                              -2003.     1172.    -1.71  8.84e- 2\n 3 seasons_Spring                        -1963.      216.    -9.10  9.11e-18\n 4 seasons_Summer                         6779.      800.     8.47  8.37e-16\n 5 seasons_Winter                        -4760.      845.    -5.64  3.79e- 8\n 6 holiday_No.Holiday                      749.      172.     4.37  1.69e- 5\n 7 day_type_Weekend                      -1091.      153.    -7.14  6.13e-12\n 8 seasons_Spring_x_holiday_No.Holiday    -260.      213.    -1.22  2.24e- 1\n 9 seasons_Summer_x_holiday_No.Holiday    -179.      232.    -0.770 4.42e- 1\n10 seasons_Winter_x_holiday_No.Holiday    -301.      168.    -1.79  7.48e- 2\n# ℹ 18 more rows"
  },
  {
    "objectID": "hw9.html#dataset-and-analysis-from-previous-assignment.",
    "href": "hw9.html#dataset-and-analysis-from-previous-assignment.",
    "title": "Assignment 9",
    "section": "",
    "text": "# Reading in data\ndata &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",\n                 local = locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n# Turning the Date column into a real Date\n# Dropping old Date column\ndata &lt;- data |&gt;\n  mutate(date = lubridate::dmy(Date)) |&gt;\n  select(-Date)\n\n# Turning character columns into factor variables\ndata &lt;- data |&gt;\n  mutate(seasons = factor(Seasons),\n         holiday = factor(Holiday),\n         fn_day = factor(`Functioning Day`)) |&gt;\n  select(-Seasons, -Holiday, -`Functioning Day`) # dropping old variables\n\n# Renaming other variables for simplicity\ndata &lt;- data |&gt;\n  rename(bike_count = \"Rented Bike Count\",\n    hour = \"Hour\",\n    temperature = \"Temperature(°C)\",\n    humidity = \"Humidity(%)\",\n    wind_speed = \"Wind speed (m/s)\",\n    visibility = \"Visibility (10m)\",\n    dew_point_temp = \"Dew point temperature(°C)\",\n    solar_radiation = \"Solar Radiation (MJ/m2)\",\n    rainfall = \"Rainfall(mm)\",\n    snowfall = \"Snowfall (cm)\")\n\n# Removing observations where buke rentals were out of commission.\ndata &lt;- data |&gt;\n  filter(fn_day == \"Yes\") |&gt;\n  select(-fn_day)\n\n# group_by() the date, seasons, and holiday variables and find the sum of the bike_count, rainfall, and snowfall variables and the mean of all the weather related variables.\ndata &lt;- data |&gt;\n  group_by(date, seasons, holiday) |&gt;\n  summarize(bike_count = sum(bike_count),\n             temp = mean(temperature),\n             humidity = mean(humidity),\n             wind_speed = mean(wind_speed),\n             vis = mean(visibility),\n             dew_point_temp = mean(dew_point_temp),\n             solar_radiation = mean(solar_radiation),\n             rainfall = sum(rainfall),\n             snowfall = sum(snowfall)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'date', 'seasons'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n# tidy models to split the data into a training and test set (75/25 split)\n# strata argument to stratify the split on the seasons\n\ndata_split &lt;- initial_split(data, prop = 0.75,strata = seasons)\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\n\n# 10 fold cross validation on the training set\ndata_10_fold &lt;- vfold_cv(data_train, 10)\n\n\n\n\n\n# Recipe 1\nrec_1 &lt;- recipe(bike_count ~ ., data = data_split) |&gt;\n  step_date(date, features = \"dow\") |&gt; # extracting \"dow\" column elements\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt; # removing unneeded variables\n  step_dummy(seasons, holiday, day_type) |&gt; # dummy variables for factors\n  step_normalize(all_numeric(), -bike_count) # normalizing all numeric variables besides response variable\n\n# Recipe 2\nrec_2 &lt;- rec_1 |&gt;\n  step_interact(terms = ~starts_with(\"seasons\")*starts_with(\"holiday\") +\n                  starts_with(\"seasons\")*temp +\n                  temp*rainfall) # Interactions for 2nd recipe model\n\n# Recipe 3\nrec_3 &lt;- rec_2 |&gt; \n  step_poly(temp,\n            wind_speed,\n            vis,\n            dew_point_temp,\n            solar_radiation,\n            rainfall,\n            snowfall,\n            degree = 2) # Quadratic terms for each numeric predictor\n\n\n\n\n\n# Linear regression model\nmlr_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n\n\n\n\n# Model 1\nmlr_fit_1 &lt;- workflow() |&gt;\n  add_recipe(rec_1) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  fit_resamples(data_10_fold) # 10 fold CV\n\n# Model 2\nmlr_fit_2 &lt;- workflow() |&gt;\n  add_recipe(rec_2) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  fit_resamples(data_10_fold) # 10 fold CV\n\n# Model 3\nmlr_fit_3 &lt;- workflow() |&gt;\n  add_recipe(rec_3) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  fit_resamples(data_10_fold) # 10 fold CV\n\n\n\n\n\n# Collecting model metrics for all three fitted MLR models\nrbind(mlr_fit_1 |&gt; collect_metrics(),\n      mlr_fit_2 |&gt; collect_metrics(),\n      mlr_fit_3 |&gt; collect_metrics())\n\n# A tibble: 6 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   3969.       10 185.     Preprocessor1_Model1\n2 rsq     standard      0.852    10   0.0125 Preprocessor1_Model1\n3 rmse    standard   3182.       10 197.     Preprocessor1_Model1\n4 rsq     standard      0.904    10   0.0114 Preprocessor1_Model1\n5 rmse    standard   3114.       10 200.     Preprocessor1_Model1\n6 rsq     standard      0.908    10   0.0115 Preprocessor1_Model1\n\n\nBest model is model 3\n\n\n\n\nmlr_final_fit &lt;- workflow() |&gt;\n  add_recipe(rec_3) |&gt; # Recipe\n  add_model(mlr_spec) |&gt; # Defined model instance\n  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\nmlr_final_fit |&gt;\n  collect_metrics() # Collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2770. Preprocessor1_Model1\n2 mae     standard       2224. Preprocessor1_Model1\n\n\n\n\n\n\n# Extract the final fitted parsnip model\nmlr_model &lt;- mlr_final_fit |&gt; extract_fit_parsnip()\n\n# View the model details\nmlr_model\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                        (Intercept)                             humidity  \n                            19906.7                              -1094.6  \n                     seasons_Spring                       seasons_Summer  \n                            -1856.4                               5649.9  \n                     seasons_Winter                   holiday_No.Holiday  \n                            -4817.2                                768.5  \n                   day_type_Weekend  seasons_Spring_x_holiday_No.Holiday  \n                            -1062.8                               -295.8  \nseasons_Summer_x_holiday_No.Holiday  seasons_Winter_x_holiday_No.Holiday  \n                             -214.0                               -362.8  \n              seasons_Spring_x_temp                seasons_Summer_x_temp  \n                             2245.9                              -6087.2  \n              seasons_Winter_x_temp                      temp_x_rainfall  \n                            -2129.0                               -621.5  \n                        temp_poly_1                          temp_poly_2  \n                           -17366.7                             -16127.7  \n                  wind_speed_poly_1                    wind_speed_poly_2  \n                            -4600.1                               1594.5  \n                         vis_poly_1                           vis_poly_2  \n                             5474.9                              -1313.6  \n              dew_point_temp_poly_1                dew_point_temp_poly_2  \n                            64361.6                              -5017.9  \n             solar_radiation_poly_1               solar_radiation_poly_2  \n                            48127.0                              -4002.4  \n                    rainfall_poly_1                      rainfall_poly_2  \n                           -42702.7                              13333.2  \n                    snowfall_poly_1                      snowfall_poly_2  \n                            -2682.3                               -336.9  \n\n\n\n\n\n\ncoefficients &lt;- broom::tidy(mlr_model)\ncoefficients \n\n# A tibble: 28 × 5\n   term                                estimate std.error statistic  p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                           19907.     1571.    12.7   2.17e-28\n 2 humidity                              -1095.     1370.    -0.799 4.25e- 1\n 3 seasons_Spring                        -1856.      260.    -7.15  1.10e-11\n 4 seasons_Summer                         5650.     1012.     5.58  6.59e- 8\n 5 seasons_Winter                        -4817.     1003.    -4.80  2.81e- 6\n 6 holiday_No.Holiday                      768.      201.     3.83  1.67e- 4\n 7 day_type_Weekend                      -1063.      185.    -5.74  2.89e- 8\n 8 seasons_Spring_x_holiday_No.Holiday    -296.      249.    -1.19  2.36e- 1\n 9 seasons_Summer_x_holiday_No.Holiday    -214.      269.    -0.795 4.27e- 1\n10 seasons_Winter_x_holiday_No.Holiday    -363.      201.    -1.80  7.25e- 2\n# ℹ 18 more rows\n\n\nFor the following models (LASSO, Regression Tree, Bagged Tree, and Random Forest) we will be using recipe 1 when fitting the model workflow."
  },
  {
    "objectID": "hw9.html#fitting-a-tuned-lasso-model",
    "href": "hw9.html#fitting-a-tuned-lasso-model",
    "title": "Assignment 9",
    "section": "",
    "text": "LASSO_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\n\n\n\n\n# LASSO model 1\nLASSO_wkf &lt;- workflow() |&gt;\n  add_recipe(rec_1) |&gt; # Recipe\n  add_model(LASSO_spec) # Defined model instance\n\n\n\n\n\nLASSO_grid &lt;- LASSO_wkf |&gt;\n  tune_grid(resamples = data_10_fold,\n            grid = grid_regular(penalty(), levels = 200),\n            metrics = metric_set(rmse, mae)) # defining model metrics\n\nWarning: package 'glmnet' was built under R version 4.4.2\n\n\n\n\n\n\nLASSO_grid |&gt;\n  collect_metrics() |&gt; # collecting defining model metrics\n  filter(.metric == \"rmse\") # filter by defining model metrics\n\n# A tibble: 200 × 7\n    penalty .metric .estimator  mean     n std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   3960.    10    189. Preprocessor1_Model001\n 2 1.12e-10 rmse    standard   3960.    10    189. Preprocessor1_Model002\n 3 1.26e-10 rmse    standard   3960.    10    189. Preprocessor1_Model003\n 4 1.41e-10 rmse    standard   3960.    10    189. Preprocessor1_Model004\n 5 1.59e-10 rmse    standard   3960.    10    189. Preprocessor1_Model005\n 6 1.78e-10 rmse    standard   3960.    10    189. Preprocessor1_Model006\n 7 2.00e-10 rmse    standard   3960.    10    189. Preprocessor1_Model007\n 8 2.25e-10 rmse    standard   3960.    10    189. Preprocessor1_Model008\n 9 2.52e-10 rmse    standard   3960.    10    189. Preprocessor1_Model009\n10 2.83e-10 rmse    standard   3960.    10    189. Preprocessor1_Model010\n# ℹ 190 more rows\n\n\n\n\n\n\nlowest_rmse &lt;- LASSO_grid |&gt;\n  select_best(metric = \"rmse\")\n\n\n\n\n\nLASSO_final_fit &lt;- LASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse) |&gt;\n  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\nLASSO_final_fit |&gt;\n  collect_metrics() # collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4547. Preprocessor1_Model1\n2 mae     standard       3528. Preprocessor1_Model1\n\n\n\n\n\n\n# Extract the final model fits\nlasso_model &lt;- LASSO_final_fit |&gt;\n  extract_fit_engine()\n\n# View the final LASSO model details\nlasso_model\n\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev Lambda\n1   0  0.00 7266.0\n2   2 12.01 6621.0\n3   2 22.33 6032.0\n4   2 30.89 5497.0\n5   3 38.17 5008.0\n6   3 44.46 4563.0\n7   3 49.68 4158.0\n8   3 54.02 3789.0\n9   3 57.62 3452.0\n10  3 60.61 3145.0\n11  3 63.09 2866.0\n12  3 65.14 2611.0\n13  3 66.85 2379.0\n14  3 68.27 2168.0\n15  3 69.45 1975.0\n16  3 70.43 1800.0\n17  4 71.90 1640.0\n18  4 73.16 1494.0\n19  4 74.20 1362.0\n20  4 75.07 1241.0\n21  4 75.79 1130.0\n22  5 76.42 1030.0\n23  5 77.29  938.5\n24  5 78.01  855.1\n25  7 78.73  779.1\n26  7 79.41  709.9\n27  7 79.98  646.8\n28  7 80.45  589.4\n29  8 80.89  537.0\n30  8 81.26  489.3\n31  8 81.57  445.8\n32  8 81.83  406.2\n33  9 82.14  370.1\n34 10 82.59  337.3\n35 10 82.97  307.3\n36 10 83.28  280.0\n37 10 83.54  255.1\n38 10 83.76  232.5\n39 10 83.94  211.8\n40 10 84.09  193.0\n41 10 84.21  175.8\n42 10 84.32  160.2\n43 10 84.40  146.0\n44 10 84.47  133.0\n45 10 84.53  121.2\n46 10 84.58  110.4\n47 10 84.62  100.6\n48 11 84.65   91.7\n49 11 84.68   83.5\n50 11 84.71   76.1\n51 11 84.73   69.4\n52 11 84.74   63.2\n53 12 84.76   57.6\n54 12 84.77   52.5\n55 12 84.78   47.8\n56 11 84.78   43.6\n57 11 84.79   39.7\n58 12 84.80   36.2\n59 12 84.80   33.0\n60 12 84.82   30.0\n61 12 84.84   27.4\n62 12 84.86   24.9\n63 12 84.87   22.7\n64 12 84.88   20.7\n65 12 84.89   18.9\n66 12 84.90   17.2\n67 12 84.91   15.7\n68 12 84.91   14.3\n69 13 84.92   13.0\n70 13 84.93   11.8\n71 13 84.93   10.8\n72 13 84.93    9.8\n73 13 84.94    9.0\n74 13 84.94    8.2\n75 13 84.94    7.4\n76 13 84.94    6.8\n77 12 84.95    6.2\n78 12 84.95    5.6\n\n\n\n\n\n\nalmost_usual_fit &lt;- extract_fit_parsnip(LASSO_final_fit)\nusual_fit &lt;- almost_usual_fit$fit\nsummary(usual_fit)\n\n          Length Class     Mode   \na0          78   -none-    numeric\nbeta      1014   dgCMatrix S4     \ndf          78   -none-    numeric\ndim          2   -none-    numeric\nlambda      78   -none-    numeric\ndev.ratio   78   -none-    numeric\nnulldev      1   -none-    numeric\nnpasses      1   -none-    numeric\njerr         1   -none-    numeric\noffset       1   -none-    logical\ncall         5   -none-    call   \nnobs         1   -none-    numeric"
  },
  {
    "objectID": "hw9.html#fitting-a-tuned-regression-tree-model",
    "href": "hw9.html#fitting-a-tuned-regression-tree-model",
    "title": "Assignment 9",
    "section": "",
    "text": "tree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n\n\n\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(rec_1) |&gt; # Recipe\n  add_model(tree_mod) # Defined model instance\n\n\n\n\n\ntree_fit &lt;- tree_wkf |&gt; \n  tune_grid(resamples = data_10_fold, # Cross-validation defined earlier\n            metrics = metric_set(rmse, mae)) # defining model metrics\n\n\n\n\n\ntree_fit |&gt;\n  collect_metrics() # collecting defined model metrics\n\n# A tibble: 20 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1        6.52e-10         11 mae     standard   2646.    10    111. Preprocess…\n 2        6.52e-10         11 rmse    standard   3575.    10    174. Preprocess…\n 3        3.42e- 9          5 mae     standard   2817.    10    125. Preprocess…\n 4        3.42e- 9          5 rmse    standard   3782.    10    179. Preprocess…\n 5        8.73e- 8          7 mae     standard   2667.    10    117. Preprocess…\n 6        8.73e- 8          7 rmse    standard   3594.    10    176. Preprocess…\n 7        1.76e- 6         11 mae     standard   2646.    10    111. Preprocess…\n 8        1.76e- 6         11 rmse    standard   3575.    10    174. Preprocess…\n 9        6.02e- 2          2 mae     standard   3524.    10    226. Preprocess…\n10        6.02e- 2          2 rmse    standard   4477.    10    231. Preprocess…\n11        3.15e- 5          2 mae     standard   3445.    10    210. Preprocess…\n12        3.15e- 5          2 rmse    standard   4400.    10    223. Preprocess…\n13        6.87e- 3          9 mae     standard   2882.    10    140. Preprocess…\n14        6.87e- 3          9 rmse    standard   3782.    10    188. Preprocess…\n15        1.74e- 5         13 mae     standard   2646.    10    111. Preprocess…\n16        1.74e- 5         13 rmse    standard   3575.    10    174. Preprocess…\n17        3.16e- 4         14 mae     standard   2646.    10    111. Preprocess…\n18        3.16e- 4         14 rmse    standard   3575.    10    174. Preprocess…\n19        8.11e- 9          5 mae     standard   2817.    10    125. Preprocess…\n20        8.11e- 9          5 rmse    standard   3782.    10    179. Preprocess…\n\n\n\n\n\n\ntree_best_params &lt;- tree_fit |&gt;\n  select_best(metric = \"rmse\") # defining the metric\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1        0.000316         14 Preprocessor1_Model09\n\n\n\n\n\n\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\n\n\n\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\ntree_final_fit |&gt;\n  collect_metrics() # Collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3799. Preprocessor1_Model1\n2 mae     standard       2966. Preprocessor1_Model1\n\n\n\n\n\n\n# final model fits\ntree_final_model &lt;- extract_workflow(tree_final_fit) \ntree_final_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 263 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n  1) root 263 24958860000 17099.720  \n    2) temp&lt; -0.2201567 113  3337065000  8759.195  \n      4) seasons_Winter&gt;=0.561781 67   209102200  5357.239  \n        8) temp&lt; -1.472334 21    23429830  4127.286 *\n        9) temp&gt;=-1.472334 46   139400900  5918.739  \n         18) holiday_No.Holiday&lt; -1.833776 7    16589840  3850.571 *\n         19) holiday_No.Holiday&gt;=-1.833776 39    87495740  6289.949  \n           38) snowfall&gt;=0.05246307 9    11765280  4952.778 *\n           39) snowfall&lt; 0.05246307 30    54810550  6691.100  \n             78) day_type_Weekend&gt;=0.4151317 11    14333350  5637.455 *\n             79) day_type_Weekend&lt; 0.4151317 19    21195320  7301.105 *\n      5) seasons_Winter&lt; 0.561781 46  1223148000 13714.220  \n       10) seasons_Spring&gt;=0.561781 24   292468100 10312.880  \n         20) rainfall&gt;=-0.2414439 8    52772870  6782.625 *\n         21) rainfall&lt; -0.2414439 16    90143270 12078.000 *\n       11) seasons_Spring&lt; 0.561781 22   350119700 17424.770  \n         22) solar_radiation&lt; -0.7186281 7   101498900 13361.000 *\n         23) solar_radiation&gt;=-0.7186281 15    79074510 19321.200 *\n    3) temp&gt;=-0.2201567 150  7839229000 23382.920  \n      6) solar_radiation&lt; -1.058026 15   231919400  7188.933 *\n      7) solar_radiation&gt;=-1.058026 135  3236556000 25182.250  \n       14) solar_radiation&lt; 0.1840252 39   700988100 21654.920  \n         28) solar_radiation&lt; -0.5000219 13   138297700 18620.540 *\n         29) solar_radiation&gt;=-0.5000219 26   383144400 23172.120  \n           58) humidity&lt; 0.7966163 19   247086800 22028.790 *\n           59) humidity&gt;=0.7966163 7    43806990 26275.430 *\n       15) solar_radiation&gt;=0.1840252 96  1853199000 26615.230  \n         30) temp&gt;=1.501809 15    80731950 21476.330 *\n         31) temp&lt; 1.501809 81  1302987000 27566.880  \n           62) temp&lt; 0.1956372 14   173282400 22392.710 *\n           63) temp&gt;=0.1956372 67   676579600 28648.040  \n            126) temp&lt; 0.5094808 18   193311100 26427.280 *\n            127) temp&gt;=0.5094808 49   361885700 29463.840  \n              254) temp&gt;=1.021364 18    48372290 27055.280 *\n              255) temp&lt; 1.021364 31   148461100 30862.350  \n                510) seasons_Summer&lt; 0.5391721 17    37981820 30034.000 *\n                511) seasons_Summer&gt;=0.5391721 14    84649850 31868.210 *\n\n\n\n\n\n\n# Extract the fitted engine \ntree_final_fit_engine &lt;- tree_final_model |&gt; extract_fit_engine()\n\n# Plot the decision tree using rpart.plot\nrpart.plot(tree_final_fit_engine, roundint = FALSE)"
  },
  {
    "objectID": "hw9.html#fitting-a-tuned-bagged-tree-model",
    "href": "hw9.html#fitting-a-tuned-bagged-tree-model",
    "title": "Assignment 9",
    "section": "",
    "text": "bag_spec &lt;- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |&gt;\n set_engine(\"rpart\") |&gt;\n set_mode(\"regression\")\n\n\n\n\n\nbag_wkf &lt;- workflow() |&gt;\n add_recipe(rec_1) |&gt; # Recipe\n add_model(bag_spec) # Defined model instance\n\n\n\n\n\nbag_fit &lt;- bag_wkf |&gt;\n tune_grid(resamples = data_10_fold, \n           grid = grid_regular(cost_complexity(), \n                               levels = 15), \n           metrics = metric_set(rmse, mae)) # defining model metrics\n\n\n\n\n\nbag_fit |&gt;\n collect_metrics() |&gt; # collecting defined model metrics\n filter(.metric == \"rmse\") |&gt;\n arrange(mean) # Arranged by mean RMSE value\n\n# A tibble: 15 × 7\n   cost_complexity .metric .estimator  mean     n std_err .config              \n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1        1.93e- 9 rmse    standard   3115.    10    169. Preprocessor1_Model03\n 2        7.20e- 7 rmse    standard   3162.    10    153. Preprocessor1_Model07\n 3        2.68e- 4 rmse    standard   3164.    10    144. Preprocessor1_Model11\n 4        3.16e- 6 rmse    standard   3201.    10    157. Preprocessor1_Model08\n 5        1.64e- 7 rmse    standard   3204.    10    132. Preprocessor1_Model06\n 6        4.39e-10 rmse    standard   3224.    10    156. Preprocessor1_Model02\n 7        8.48e- 9 rmse    standard   3229.    10    159. Preprocessor1_Model04\n 8        1.18e- 3 rmse    standard   3233.    10    175. Preprocessor1_Model12\n 9        6.11e- 5 rmse    standard   3240.    10    152. Preprocessor1_Model10\n10        1.39e- 5 rmse    standard   3245.    10    115. Preprocessor1_Model09\n11        1   e-10 rmse    standard   3251.    10    118. Preprocessor1_Model01\n12        3.73e- 8 rmse    standard   3258.    10    166. Preprocessor1_Model05\n13        5.18e- 3 rmse    standard   3347.    10    140. Preprocessor1_Model13\n14        2.28e- 2 rmse    standard   3805.    10    184. Preprocessor1_Model14\n15        1   e- 1 rmse    standard   4807.    10    195. Preprocessor1_Model15\n\n\n\n\n\n\nbag_best_params &lt;- bag_fit |&gt;\n  select_best(metric = \"rmse\")\nbag_best_params\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1   0.00000000193 Preprocessor1_Model03\n\n\n\n\n\n\nbag_final_wkf &lt;- bag_wkf |&gt;\n finalize_workflow(bag_best_params)\n\n\n\n\n\nbag_final_fit &lt;- bag_final_wkf |&gt;\n last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\n\nbag_final_fit |&gt;\n  collect_metrics() # Collecting defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3692. Preprocessor1_Model1\n2 mae     standard       3022. Preprocessor1_Model1\n\n\n\n\n\n\n# final model fits\nbag_final_model &lt;- extract_fit_engine(bag_final_fit) \nbag_final_model\n\nBagged CART (regression with 11 members)\n\nVariable importance scores include:\n\n# A tibble: 13 × 4\n   term                      value  std.error  used\n   &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n 1 temp               15443136673. 454365045.    11\n 2 solar_radiation    12633999997. 496795895.    11\n 3 dew_point_temp     12426918314. 589638733.    11\n 4 seasons_Winter      9449409307. 518154069.    11\n 5 humidity            7510558669. 505634340.    11\n 6 rainfall            4021505847. 274143212.    11\n 7 wind_speed          2182743273. 448114758.    11\n 8 seasons_Summer      1601058133. 644754532.     8\n 9 vis                 1584063189. 161475765.    11\n10 snowfall            1210680245. 442348352.    10\n11 seasons_Spring      1192847994. 166328461.    11\n12 day_type_Weekend     246104025. 177253467.    11\n13 holiday_No.Holiday    44282323.  29462086.     6\n\n\n\n\n\n\n# Creating variable importance plot\nbag_final_model$imp |&gt;\n mutate(term = factor(term, levels = term)) |&gt;\n ggplot(aes(x = term, y = value)) +\n geom_bar(stat =\"identity\") +\n coord_flip() +\n  labs(title = \"Variable Importance for Bagged Tree Model\",\n       x = \"Features\",\n       y = \"Importance\")"
  },
  {
    "objectID": "hw9.html#fitting-a-tuned-random-forest-model",
    "href": "hw9.html#fitting-a-tuned-random-forest-model",
    "title": "Assignment 9",
    "section": "",
    "text": "rf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n set_engine(\"ranger\", importance = \"impurity\") |&gt;\n set_mode(\"regression\")\n\n\n\n\n\nrf_wkf &lt;- workflow() |&gt;\n add_recipe(rec_1) |&gt; # Recipe\n add_model(rf_spec) # Defined model instance\n\n\n\n\n\nrf_fit &lt;- rf_wkf |&gt;\n tune_grid(resamples = data_10_fold,\n grid = 7,\n metrics = metric_set(rmse, mae)) # defining model metrics\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n\n\n\n\nrf_fit |&gt;\n collect_metrics() |&gt; # Collecting defined model metrics\n filter(.metric == \"rmse\") |&gt;\n arrange(mean) # Arranged by mean RMSE value\n\n# A tibble: 7 × 7\n   mtry .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1    13 rmse    standard   2879.    10    124. Preprocessor1_Model6\n2    11 rmse    standard   2897.    10    115. Preprocessor1_Model2\n3     9 rmse    standard   2911.    10    115. Preprocessor1_Model3\n4     7 rmse    standard   2950.    10    123. Preprocessor1_Model5\n5     6 rmse    standard   2989.    10    123. Preprocessor1_Model1\n6     3 rmse    standard   3237.    10    134. Preprocessor1_Model4\n7     2 rmse    standard   3503.    10    152. Preprocessor1_Model7\n\n\n\n\n\n\nrf_best_params &lt;- rf_fit |&gt;\n  select_best(metric = \"rmse\") # defining the metric\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1    13 Preprocessor1_Model6\n\n\n\n\n\n\nrf_final_wkf &lt;- rf_wkf |&gt;\n finalize_workflow(rf_best_params)\n\n\n\n\n\nrf_final_fit &lt;- rf_final_wkf |&gt;\n last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics\nrf_final_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [263/90]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\nrf_final_fit |&gt;\n  collect_metrics() # Collecting by defined model metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2859. Preprocessor1_Model1\n2 mae     standard       2348. Preprocessor1_Model1\n\n\n\n\n\n\n# final model fits\nrf_final_model &lt;- extract_fit_engine(rf_final_fit) \nrf_final_model\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~13L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      263 \nNumber of independent variables:  13 \nMtry:                             13 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       8174479 \nR squared (OOB):                  0.9141902 \n\n\n\n\n\n\n# Extract variable importance and convert to a tibble\nrf_imp &lt;- rf_final_model$variable.importance\nrf_imp_df &lt;- tibble(\n  term = names(rf_imp), # Feature names\n  value = rf_imp # Feature importance values\n)\n\n# Creating variable importance plot\nrf_imp_df |&gt;\n  mutate(term = factor(term, levels = term)) |&gt;\n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Variable Importance for Random Forest Model\",\n       x = \"Features\",\n       y = \"Importance\")"
  },
  {
    "objectID": "hw9.html#comparing-all-final-models-using-root-mean-squared-error-rmse-and-mean-absolute-error-mae",
    "href": "hw9.html#comparing-all-final-models-using-root-mean-squared-error-rmse-and-mean-absolute-error-mae",
    "title": "Assignment 9",
    "section": "",
    "text": "# MLR model \nmlr_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2770. Preprocessor1_Model1\n2 mae     standard       2224. Preprocessor1_Model1\n\n\n\n# tuned LASSO Model\nLASSO_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4547. Preprocessor1_Model1\n2 mae     standard       3528. Preprocessor1_Model1\n\n\n\n# tuned Regression Tree Model\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3799. Preprocessor1_Model1\n2 mae     standard       2966. Preprocessor1_Model1\n\n\n\n# tuned Bagged Tree Model\nbag_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3692. Preprocessor1_Model1\n2 mae     standard       3022. Preprocessor1_Model1\n\n\n\n# tuned Random Forest Model\nrf_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2859. Preprocessor1_Model1\n2 mae     standard       2348. Preprocessor1_Model1\n\n\nIn comparison of all final models: the Multiple Linear Model is the best!\nFYI: I didn’t set a seed so every time I render my quarto document the best final model changes between the MLR and Random Forest models. Most of the time, the MLR model is the best."
  },
  {
    "objectID": "hw9.html#fitting-the-best-overall-model-to-the-entire-dataset",
    "href": "hw9.html#fitting-the-best-overall-model-to-the-entire-dataset",
    "title": "Assignment 9",
    "section": "",
    "text": "# Fit the model to the entire dataset\nMLR_final &lt;- workflow() |&gt;\n  add_recipe(rec_3) |&gt; # Recipe\n  add_model(mlr_spec) |&gt;\n  fit(data)\n\nMLR_final |&gt;\n  tidy()\n\n# A tibble: 28 × 5\n   term                                estimate std.error statistic  p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                           20922.     1288.    16.2   7.52e-44\n 2 humidity                              -2003.     1172.    -1.71  8.84e- 2\n 3 seasons_Spring                        -1963.      216.    -9.10  9.11e-18\n 4 seasons_Summer                         6779.      800.     8.47  8.37e-16\n 5 seasons_Winter                        -4760.      845.    -5.64  3.79e- 8\n 6 holiday_No.Holiday                      749.      172.     4.37  1.69e- 5\n 7 day_type_Weekend                      -1091.      153.    -7.14  6.13e-12\n 8 seasons_Spring_x_holiday_No.Holiday    -260.      213.    -1.22  2.24e- 1\n 9 seasons_Summer_x_holiday_No.Holiday    -179.      232.    -0.770 4.42e- 1\n10 seasons_Winter_x_holiday_No.Holiday    -301.      168.    -1.79  7.48e- 2\n# ℹ 18 more rows"
  }
]