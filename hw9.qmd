---
title: "Assignment 9"
author: "Ethan Marburger"
format: html
editor: visual
---

# Modeling Practice

```{r}
#| echo: false
#| include: false
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(dplyr)
library(parsnip)
library(tune)
library(ggcorrplot)
library(tree)
library(baguette)
library(ranger)
library(rpart.plot)
```

## Dataset and analysis from previous assignment.

### The following code is from Assignment 8 where we fit multiple linear regression models using 10 fold cross-validation. 

```{r}
# Reading in data
data <- read_csv("https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv",
                 local = locale(encoding = "latin1"))
```

### Preparing the data for analysis.

```{r}
# Turning the Date column into a real Date
# Dropping old Date column
data <- data |>
  mutate(date = lubridate::dmy(Date)) |>
  select(-Date)

# Turning character columns into factor variables
data <- data |>
  mutate(seasons = factor(Seasons),
         holiday = factor(Holiday),
         fn_day = factor(`Functioning Day`)) |>
  select(-Seasons, -Holiday, -`Functioning Day`) # dropping old variables

# Renaming other variables for simplicity
data <- data |>
  rename(bike_count = "Rented Bike Count",
    hour = "Hour",
    temperature = "Temperature(°C)",
    humidity = "Humidity(%)",
    wind_speed = "Wind speed (m/s)",
    visibility = "Visibility (10m)",
    dew_point_temp = "Dew point temperature(°C)",
    solar_radiation = "Solar Radiation (MJ/m2)",
    rainfall = "Rainfall(mm)",
    snowfall = "Snowfall (cm)")

# Removing observations where buke rentals were out of commission.
data <- data |>
  filter(fn_day == "Yes") |>
  select(-fn_day)

# group_by() the date, seasons, and holiday variables and find the sum of the bike_count, rainfall, and snowfall variables and the mean of all the weather related variables.
data <- data |>
  group_by(date, seasons, holiday) |>
  summarize(bike_count = sum(bike_count),
             temp = mean(temperature),
             humidity = mean(humidity),
             wind_speed = mean(wind_speed),
             vis = mean(visibility),
             dew_point_temp = mean(dew_point_temp),
             solar_radiation = mean(solar_radiation),
             rainfall = sum(rainfall),
             snowfall = sum(snowfall)) |>
  ungroup()
```

### Splitting the data into training/test sets

```{r}
# tidy models to split the data into a training and test set (75/25 split)
# strata argument to stratify the split on the seasons

data_split <- initial_split(data, prop = 0.75,strata = seasons)
data_train <- training(data_split)
data_test <- testing(data_split)

# 10 fold cross validation on the training set
data_10_fold <- vfold_cv(data_train, 10)
```

### Fitting MLR Models

```{r}
# Recipe 1
rec_1 <- recipe(bike_count ~ ., data = data_split) |>
  step_date(date, features = "dow") |> # extracting "dow" column elements
  step_mutate(day_type = factor(if_else(date_dow %in% c("Sat", "Sun"), "Weekend", "Weekday"))) |>
  step_rm(date, date_dow) |> # removing unneeded variables
  step_dummy(seasons, holiday, day_type) |> # dummy variables for factors
  step_normalize(all_numeric(), -bike_count) # normalizing all numeric variables besides response variable

# Recipe 2
rec_2 <- rec_1 |>
  step_interact(terms = ~starts_with("seasons")*starts_with("holiday") +
                  starts_with("seasons")*temp +
                  temp*rainfall) # Interactions for 2nd recipe model

# Recipe 3
rec_3 <- rec_2 |> 
  step_poly(temp,
            wind_speed,
            vis,
            dew_point_temp,
            solar_radiation,
            rainfall,
            snowfall,
            degree = 2) # Quadratic terms for each numeric predictor
```

### Creating a linear regression instance

```{r}
# Linear regression model
mlr_spec <- linear_reg() |>
  set_engine("lm")
```

### Fitting the models via workflows()!

```{r}
# Model 1
mlr_fit_1 <- workflow() |>
  add_recipe(rec_1) |> # Recipe
  add_model(mlr_spec) |> # Defined model instance
  fit_resamples(data_10_fold) # 10 fold CV

# Model 2
mlr_fit_2 <- workflow() |>
  add_recipe(rec_2) |> # Recipe
  add_model(mlr_spec) |> # Defined model instance
  fit_resamples(data_10_fold) # 10 fold CV

# Model 3
mlr_fit_3 <- workflow() |>
  add_recipe(rec_3) |> # Recipe
  add_model(mlr_spec) |> # Defined model instance
  fit_resamples(data_10_fold) # 10 fold CV
```

### Obtaining MLR model metrics

```{r}
# Collecting model metrics for all three fitted MLR models
rbind(mlr_fit_1 |> collect_metrics(),
      mlr_fit_2 |> collect_metrics(),
      mlr_fit_3 |> collect_metrics())
```

Best model is model 3

### Fitting best MLR model (model 3) to the entire training set.

```{r}
mlr_final_fit <- workflow() |>
  add_recipe(rec_3) |> # Recipe
  add_model(mlr_spec) |> # Defined model instance
  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics

mlr_final_fit |>
  collect_metrics() # Collecting defined model metrics
```

### Extracting final model fit for the best MLR model

```{r}
# Extract the final fitted parsnip model
mlr_model <- mlr_final_fit |> extract_fit_parsnip()

# View the model details
mlr_model
```

### Reporting the final MLR coefficients table

```{r}
coefficients <- broom::tidy(mlr_model)
coefficients 
```

**For the following models (LASSO, Regression Tree, Bagged Tree, and Random Forest) we will be using recipe 1 when fitting the model workflow.**

## Fitting a tuned LASSO model

### Creating a LASSO model instance

```{r}
LASSO_spec <- linear_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")
```

### LASSO Workflows

```{r}
# LASSO model 1
LASSO_wkf <- workflow() |>
  add_recipe(rec_1) |> # Recipe
  add_model(LASSO_spec) # Defined model instance
```

### Fit Model with tune_grid() and grid_regular()

```{r}
LASSO_grid <- LASSO_wkf |>
  tune_grid(resamples = data_10_fold,
            grid = grid_regular(penalty(), levels = 200),
            metrics = metric_set(rmse, mae)) # defining model metrics
```

### Collecting the metrics computed across the folds for each tuning parameter

```{r}
LASSO_grid |>
  collect_metrics() |> # collecting defining model metrics
  filter(.metric == "rmse") # filter by defining model metrics
```

### Using select_best() to pull out best LASSO model

```{r}
lowest_rmse <- LASSO_grid |>
  select_best(metric = "rmse")
```

### fit best LASSO model to the entire training set to see the model fit
```{r}
LASSO_final_fit <- LASSO_wkf |>
  finalize_workflow(lowest_rmse) |>
  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics

LASSO_final_fit |>
  collect_metrics() # collecting defined model metrics
```

### Extracting the model fits for the LASSO model

```{r}
# Extract the final model fits
lasso_model <- LASSO_final_fit |>
  extract_fit_engine()

# View the final LASSO model details
lasso_model
```

### Reporting the final LASSO coefficients table

```{r}
almost_usual_fit <- extract_fit_parsnip(LASSO_final_fit)
usual_fit <- almost_usual_fit$fit
summary(usual_fit)
```

## Fitting a tuned Regression Tree model

### Creating a Regression Tree Instance

```{r}
tree_mod <- decision_tree(tree_depth = tune(),
                          min_n = 20,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("regression")
```

### Regression tree workflow

```{r}
tree_wkf <- workflow() |>
  add_recipe(rec_1) |> # Recipe
  add_model(tree_mod) # Defined model instance
```

### CV to Select our Tuning Parameters

```{r}
tree_fit <- tree_wkf |> 
  tune_grid(resamples = data_10_fold, # Cross-validation defined earlier
            metrics = metric_set(rmse, mae)) # defining model metrics
```

### Pulling out model metrics 

```{r}
tree_fit |>
  collect_metrics() # collecting defined model metrics
```

### Using select_best() to grab the best models tuning parameter values

```{r}
tree_best_params <- tree_fit |>
  select_best(metric = "rmse") # defining the metric
tree_best_params
```

### Finalizing our model on the training set by fitting this chosen model via finalize_workflow()

```{r}
tree_final_wkf <- tree_wkf |>
  finalize_workflow(tree_best_params)
```

### Fitting the best model to the entire training data set to test on the testing set

```{r}
tree_final_fit <- tree_final_wkf |>
  last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics

tree_final_fit |>
  collect_metrics() # Collecting defined model metrics
```

### Extracting final model fit for the Regression Tree model

```{r}
# final model fits
tree_final_model <- extract_workflow(tree_final_fit) 
tree_final_model
```

### Plot of the final fit

```{r}
# Extract the fitted engine 
tree_final_fit_engine <- tree_final_model |> extract_fit_engine()

# Plot the decision tree using rpart.plot
rpart.plot(tree_final_fit_engine, roundint = FALSE)
```

## Fitting a tuned Bagged Tree model

### Creating a Bagged Tree model Instance

```{r}
bag_spec <- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |>
 set_engine("rpart") |>
 set_mode("regression")
```

### Bagged Tree workflow

```{r}
bag_wkf <- workflow() |>
 add_recipe(rec_1) |> # Recipe
 add_model(bag_spec) # Defined model instance
```

### CV to Select our Tuning Parameters

```{r}
bag_fit <- bag_wkf |>
 tune_grid(resamples = data_10_fold, 
           grid = grid_regular(cost_complexity(), 
                               levels = 15), 
           metrics = metric_set(rmse, mae)) # defining model metrics
```

### Pulling out model metrics

```{r}
bag_fit |>
 collect_metrics() |> # collecting defined model metrics
 filter(.metric == "rmse") |>
 arrange(mean) # Arranged by mean RMSE value

```

### Using select_best() to grab the best models tuning parameter values

```{r}
bag_best_params <- bag_fit |>
  select_best(metric = "rmse")
bag_best_params
```

### Finalizing our model on the training set by fitting this chosen model via finalize_workflow()

```{r}
bag_final_wkf <- bag_wkf |>
 finalize_workflow(bag_best_params)
```

### Fitting the best model to the entire training data set to test on the testing set

```{r}
bag_final_fit <- bag_final_wkf |>
 last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics

bag_final_fit |>
  collect_metrics() # Collecting defined model metrics
```

### Extracting the final model fit for the Bagged Tree model

```{r}
# final model fits
bag_final_model <- extract_fit_engine(bag_final_fit) 
bag_final_model
```

### Producing a variable importance plot for the Bagged Tree model

```{r}
# Creating variable importance plot
bag_final_model$imp |>
 mutate(term = factor(term, levels = term)) |>
 ggplot(aes(x = term, y = value)) +
 geom_bar(stat ="identity") +
 coord_flip() +
  labs(title = "Variable Importance for Bagged Tree Model",
       x = "Features",
       y = "Importance")
```

## Fitting a tuned Random Forest model

### Creating a Random Forest Instance

```{r}
rf_spec <- rand_forest(mtry = tune()) |>
 set_engine("ranger", importance = "impurity") |>
 set_mode("regression")
```

### Random Forest workflow

```{r}
rf_wkf <- workflow() |>
 add_recipe(rec_1) |> # Recipe
 add_model(rf_spec) # Defined model instance
```

### CV to Select our Tuning Parameters

```{r}
rf_fit <- rf_wkf |>
 tune_grid(resamples = data_10_fold,
 grid = 7,
 metrics = metric_set(rmse, mae)) # defining model metrics
```

### Pulling out model metrics

```{r}
rf_fit |>
 collect_metrics() |> # Collecting defined model metrics
 filter(.metric == "rmse") |>
 arrange(mean) # Arranged by mean RMSE value
```

### Using select_best() to grab the best models tuning parameter values

```{r}
rf_best_params <- rf_fit |>
  select_best(metric = "rmse") # defining the metric
rf_best_params
```

### Finalizing our model on the training set by fitting this chosen model via finalize_workflow()

```{r}
rf_final_wkf <- rf_wkf |>
 finalize_workflow(rf_best_params)
```

### Fitting the best model to the entire training data set to test on the testing set

```{r}
rf_final_fit <- rf_final_wkf |>
 last_fit(data_split, metrics = metric_set(rmse, mae)) # defining model metrics
rf_final_fit

rf_final_fit |>
  collect_metrics() # Collecting by defined model metrics
```

### Extracting the final model fit for the Random Forest model

```{r}
# final model fits
rf_final_model <- extract_fit_engine(rf_final_fit) 
rf_final_model
```

### Producing a variable importance plot for the Random Forest model

```{r}
# Extract variable importance and convert to a tibble
rf_imp <- rf_final_model$variable.importance
rf_imp_df <- tibble(
  term = names(rf_imp), # Feature names
  value = rf_imp # Feature importance values
)

# Creating variable importance plot
rf_imp_df |>
  mutate(term = factor(term, levels = term)) |>
  ggplot(aes(x = term, y = value)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Variable Importance for Random Forest Model",
       x = "Features",
       y = "Importance")
```
## Comparing all final models using Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE)

```{r}
# MLR model 
mlr_final_fit |>
  collect_metrics()
```

```{r}
# tuned LASSO Model
LASSO_final_fit |>
  collect_metrics()
```

```{r}
# tuned Regression Tree Model
tree_final_fit |>
  collect_metrics()
```

```{r}
# tuned Bagged Tree Model
bag_final_fit |>
  collect_metrics()
```

```{r}
# tuned Random Forest Model
rf_final_fit |>
  collect_metrics()
```

**In comparison of all final models: the Multiple Linear Model is the best!**

**FYI: I didn't set a seed so every time I render my quarto document the best final model changes between the MLR and Random Forest models. Most of the time, the MLR model is the best.**

## Fitting the best overall model to the entire dataset!

```{r}
# Fit the model to the entire dataset
MLR_final <- workflow() |>
  add_recipe(rec_3) |> # Recipe
  add_model(mlr_spec) |>
  fit(data)

MLR_final |>
  tidy()
```
